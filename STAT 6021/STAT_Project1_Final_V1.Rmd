---
output: pdf_document
---


\begin{titlepage} % Suppresses headers and footers on the title page

	\centering % Centre everything on the title page
	
	\scshape % Use small caps for all text on the title page
	
	\vspace*{\baselineskip} % White space at the top of the page
	
	%------------------------------------------------
	%	Title
	%------------------------------------------------
	
	\rule{\textwidth}{1.6pt}\vspace*{-\baselineskip}\vspace*{2pt} % Thick horizontal rule
	\rule{\textwidth}{0.4pt} % Thin horizontal rule
	
	\vspace{0.75\baselineskip} % Whitespace above the title
	
	{\LARGE STAT 6021: Project 1} 
	
	\vspace{0.5\baselineskip}
	
	{\LARGE Estimating the Price of a Diamond Based on its Characteristics} % Title
	
	\vspace{0.75\baselineskip} % Whitespace below the title
	
	\rule{\textwidth}{0.4pt}\vspace*{-\baselineskip}\vspace{3.2pt} % Thin horizontal rule
	\rule{\textwidth}{1.6pt} % Thick horizontal rule
	
	\vspace{2\baselineskip} % Whitespace after the title block
	

	
	\vspace*{3\baselineskip} % Whitespace under the subtitle
	
	%------------------------------------------------
	%	Editor(s)
	%------------------------------------------------
	
	Written By
	
	\vspace{0.5\baselineskip} % Whitespace before the editors
	\vspace{0.5\baselineskip} % Whitespace before the editors
	\vspace{0.5\baselineskip} % Whitespace before the editors
	\vspace{0.5\baselineskip} % Whitespace before the editors
	\vspace{0.5\baselineskip} % Whitespace before the editors
	\vspace{0.5\baselineskip} % Whitespace before the editors
	
	
	{\scshape\Large Ashley Scurlock \\ Cory Clayton \\ Congxin (David) Xu \\ Yibo Wang \\} % Editor list
	
	\vspace{0.5\baselineskip} % Whitespace 
	
	\vspace{0.5\baselineskip} % Whitespace 
	\vspace{0.5\baselineskip} % Whitespace 
	\vspace{0.5\baselineskip} % Whitespace 
	\vspace{0.5\baselineskip} % Whitespace 
	\vspace{0.5\baselineskip} % Whitespace
	\vspace{0.5\baselineskip} % Whitespace 
	\vspace{0.5\baselineskip} % Whitespace 
	\vspace{0.5\baselineskip} % Whitespace 
	\vspace{0.5\baselineskip} % Whitespace 
	\vspace{0.5\baselineskip} % Whitespace 
	\vspace{0.5\baselineskip} % Whitespace
	\vspace{0.5\baselineskip} % Whitespace 
	\vspace{0.5\baselineskip} % Whitespace 
	\vspace{0.5\baselineskip} % Whitespace 
	\vspace{0.5\baselineskip} % Whitespace 
	\vspace{0.5\baselineskip} % Whitespace 
	\vspace{0.5\baselineskip} % Whitespace
	\vspace{0.5\baselineskip} % Whitespace 
	\vspace{0.5\baselineskip} % Whitespace 
	\vspace{0.5\baselineskip} % Whitespace 
	\vspace{0.5\baselineskip} % Whitespace 
	\vspace{0.5\baselineskip} % Whitespace 
	\vspace{0.5\baselineskip} % Whitespace
	\vspace{0.5\baselineskip} % Whitespace 
	
	\textit{University of Virginia \\ School of Data Science} 


\end{titlepage}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(RColorBrewer)
library(datasets)
library(forecast)
library(tidyverse)
library(MASS)
library(leaps)
library(knitr)
library(Metrics)
data <- read_csv("C:\\David\\UVA\\Summer 2020\\STAT 6021\\Project 1\\clean_diamond_data.csv")
```
# Executive Summary
\large 
Diamonds are precious and they are the ideal gifts for our loved ones. However, many people had spent way more than they should on purchasing diamonds. We want to build a model that estimate the price of a diamond, so that people can develop a reasonable budget based on their requirements of the diamond. The main goal of this project is to accurately predict the price of a diamond based on its characteristics. We used the data from more than 210,000 different diamonds sold on *bluenile.com*. The characteristics of the diamonds include the weight, clarity, color and style. In order to find the best model, we applied 3 different statistical techniques and built 3 candidate models. At the end, we calculated forecast accuracy metrics for all 3 models and selected the best model with the largest prediction power.  \newline


We began with an exploratory data analysis where we studied the patterns and distribution of the data. Based on the scatterplot we drew, we can visually see that as the clarity and color of diamonds become better. the price of diamonds is very likely to increase. However, there is no clear relationship between the price and the way how diamonds are cut. In addition, we also found that more than 94% of our data weight less than 2 carats. To maintain the validity of our models, we decided to remove all data points with weight greater than or equal to 2 carats and build models only on data with weight less than 2 carats. In addition, we randomly selected 75% of the data as the training data to build the models and used the rest 25% of the data as the testing data to validate our model. This is a common practice in the data science field to further ensure the validity of our model. \newline


We build 3 different models to predict the price of a diamond. The first model is a simple linear regression model where we only use the weight to predict the price of a diamond. The second model is a multiple linear regression model where we take all the characteristics of the diamond into consideration. Our final model is based on our second model and we studied the effect of interaction terms. After comparing the forecast accuracy metrics of each model, the best model is: \newline
$$
\begin{aligned}
log(price) &= 6.830423(\beta_{carat})^{1/3} -0.226630\beta_{clarityIF} -0.573925 \beta_{claritySI1} \\
&- 0.714277\beta_{claritySI2} -0.413591\beta_{clarityVS1} - 0.466890 \beta_{clarityVS2}\\ 
&- 0.301286\beta_{clarityVVS1} - 0.365513 \beta_{clarityVVS2} - 0.063282\beta_{colorE}  \\
&- 0.088755\beta_{colorF} -0.142225\beta_{colorG} - 0.198601\beta_{colorH}\\ 
&- 0.297845\beta_{colorI} -0.420041\beta_{colorJ}-0.284020\beta_{cutGood}\\ &-0.093465\beta_{cutIdeal}-0.246152\beta_{cutVery Good} + 2.436036
\end{aligned}
$$
\newline
After reading our report, we hope that customers who wish to purchase diamonds will have a better understanding about the estimated of price of a diamonds based on its characteristics and they can refer to our price estimate to find the best time to make that purchase.

# Exploratory Data Analysis
Within our exploratory data analysis, we first perform a preliminary analysis on 100% of the diamond data. We begin with a preview of the first 6 lines of the data, followed by a summary statistics table.

```{r Preview, echo = FALSE, comment=""}
x <- as.data.frame(head(data))
kable(x, caption = 'Data Preview')


kable(summary(data[,c(1,5)]), caption = 'Summary Statistics') 

```
From the results in Table 1, we can see that we have 2 numeric variables "carat" and "price" and 3 categorical variable "clarity", "color" and "cut". The price of the diamonds will be the response variable for the project and the rest of the variables will be candidates of predictors in our models. Based on the summary statistics in Table 2, we can see that "carat" ranges from 0.23 to 20.45 with mean = 0.7621 and 3rd quartile = 1. "price" ranges from 229 to 2,317,596 with mean = 5540. Without doing any additional calculations or visualizations, we can expect to see some outliers with high values of the 3rd quartile for "carat" and "price". This will likely to create issues and bias the coefficients in our regression analysis.
\newline
\newline
To further explore patterns and distributions within this dataset, we create 3 scatterplot of "price" against "carat" by "clarity", by "color" and by "cut." 

```{r, echo = FALSE, out.width='50%', fig.show = 'hold', results='hide',fig.keep='all', warning=FALSE}
# Scatter Plot of Price vs. Carat, by Clarity
data$clarity <-
  factor(
    data$clarity,
    level = c('SI1', 'SI2', 'VS1', 'VS2', 'VVS1', 'VVS2', 'IF', 'FL'),
    label = c('SI1', 'SI2', 'VS1', 'VS2', 'VVS1', 'VVS2', 'IF', 'FL')
  )

ggplot(data = data) +
  geom_point(aes(x = carat, y = price, color = clarity)) +
  scale_color_brewer(palette = 'Spectral') +
  ggtitle("Scatter Plot of Price vs. Carat, by Clarity")

# Scatter Plot of Price vs. Carat, by Cut
data$cut <-
  factor(
    data$cut,
    level = c('Good', 'Very Good', 'Ideal', 'Astor Ideal'),
    label = c('Good', 'Very Good', 'Ideal', 'Astor Ideal')
  )

ggplot(data = data) +
  geom_point(aes(x = carat, y = price, color = cut)) +
  scale_color_brewer(palette = 'Spectral') +
  ggtitle("Scatter Plot of Price vs. Carat, by Cut")

# Scatter Plot of Price vs. Carat, by Color
data$color <-
  factor(
    data$color,
    level = c('J', 'I', 'H', 'G', 'F', 'E', 'D'),
    label = c('J', 'I', 'H', 'G', 'F', 'E', 'D')
  )

ggplot(data = data) +
  geom_point(aes(x = carat, y = price, color = color)) +
  scale_color_brewer(palette='Spectral') +
  ggtitle("Scatter Plot of Price vs. Carat, by Color")

# Density Plot by Carat
ggplot(data, aes(carat)) +
  geom_density() + 
  ggtitle("Density Plot by Carat") +
  geom_vline(xintercept = 2, color = 'red')
```

All 3 scatterplots are graphed from the lowest rating of that category to the highest rating of that category. The first scatterplot (upper-left) of Price against Carat by Clarity shows that as the clarity rating goes from "SI1" to "FL", the price gradually increases as the color of the data points change from red to blue. The story is the same for in the third scatterplot (bottom-left) of Price against Carat by Color, as we can see that the price gradually increases as the the color of the diamond goes from "J" to "D". However, in the second scatterplot (upper-right) of Price against Carat by Cut, there is no clear pattern in change of price when we move from one type of cut to another. Therefore, based on these scatterplots, we expect that clarity and color are positively correlated with price. 
\newline
\newline
In addition, we also noticed that most of our data are clustered around carat less than or equal to 2. Therefore, we also created a Density plot where we see on the bottom-right corner of the figure. The red line in the graph is a cut-off point where carat equals to 2. Based on the density plot and additional calculation in R, we found that approximately 94% of the diamonds in our data weight less than 2 carats. From an consumer perspective, ordinary people normally are not likely to look for diamonds with weight more than 2 carats. People who actually want diamonds with more than 2 carats are very likely do not care for the price of such diamonds. Therefore, in order to build a model with more prediction power, we, as a group, decide to remove all the data points with carat greater than or equal to 2. 


# Detailed Analysis
For this project, we first randomly split our data into a training dataset with 75% of the original data and a testing dataset with the rest of 25% of the data. The main purpose of doing this is to prevent overfitting and generate a out-of-sample accuracy metrics so that we can compare the forecast accuracy among different models. The 3 models we built are: \newline
1. Simple Linear Regression 
2. Multiple Linear Regression with categorical variables only
3. Multiple Linear Regression with interaction terms

## Simple Linear Regression
The first sets of models we tried are simple linear regression models. 

```{r, include=FALSE}
data <- read.csv('clean_diamond_data_train.csv')
attach(data)
rm(x)
gc()

subgroup = subset(data, carat < 2)
set.seed(42)
rows <- sample(nrow(subgroup))
data.r <- subgroup[rows,]

detach(data)

rm(results, rows)
gc()

attach(data.r)
```

```{r, echo=FALSE, comment=""}
reduced <- lm(subgroup$price ~ subgroup$carat)
summary(reduced)
```

After restricting the data to only diamonds smaller than 2 carats, we can see that there is a clear positive correlation between price and carat of the diamond. Fitting a linear regression between the two variables gives us the equation $price=8943*carat-2900$ which has an $R^2$ of 0.794 and a significant coefficients for the variable carat. The standard model are reasonably good, but as we can see in the residual plot of y (upper-left) below, there is clearly a curvature pattern. The variance is also increasing as the carat gets bigger suggesting we should transform the response first, then the predictor. \newline

```{r, echo=FALSE, comment=""}
reduced.log <- lm(log(price) ~ carat)
#summary(reduced.log)

srcarat <- sqrt(carat)
crcarat <- carat ^ (1 / 3)
reduced.log.sr <- lm(log(price) ~ srcarat)
#summary(reduced.log.sr)
```
 
The Box Cox transformation plot (upper-right) to help transform the response gave a value of lambda close to 0, so a log transformation of the response was appropriate. Running a linear regression with the log of the price gave the equation $log(price) = 2.691 \cdot carat + 5.673$ or it can be expressed as $price = 290.9 \cdot e^{2.691 \cdot carat}$. This model suggests that for every tenth of a carat size the diamond price increases by 31%. This model also greatly improved the $R^2$ going from 0.794 in the untransformed model to 0.893. The residual plot of $log(y)$ (bottom-left) looks much better through the bulk of the data but still doesn't align at the end. The variance looks constant, but doesn't have a mean value of 0 for the entire span of fitted values, suggesting a transformation of the predictor would be helpful.

```{r, echo = FALSE, out.width='50%', fig.show = 'hold', results='hide',fig.keep='all', warning=FALSE}
par(mfrow = c(1, 1))

tmp <- function() {
  plot(reduced$fitted.values, reduced$residuals, main = "Residual Plot of y")
  abline(h = 0, col = "red")
}
tmp()

boxcox(reduced)

tmp <- function() {
  plot(reduced.log$fitted.values, reduced.log$residuals, main = "Residuals plot of log(y)")
  abline(h = 0, col = "red")
}
tmp()
```

After trying a couple different options, the cubic root of the carat of the diamond gave best looking scatterplot and residual plot. It also improved the $R^2$ to 0.933 and the residuals look to have a constant variance and mean of 0.

```{r, echo=FALSE, comment=""}
reduced.log.cr <- lm(log(price) ~ crcarat)
summary(reduced.log.cr)
#confint(reduced.log.cr,level = 0.95)
```

```{r,  echo = FALSE, out.width='50%', fig.show = 'hold', results='hide', fig.keep='all', warning=FALSE}
par(mfrow = c(1, 1))
tmp <- function() {
  plot(reduced.log.cr$fitted.values,reduced.log.cr$residuals, main="Residual Plot of log(y) and cuberoot(x)")
  abline(h=0,col="red")
}
tmp()

acf(reduced.log.cr$residuals,
    main = "ACF of Residuals",
    ylim = c(-0.01, 0.01))
tmp <- function() {
  qqnorm(reduced.log.cr$residuals)
  qqline(reduced.log.cr$residuals, col = "red")
}
tmp()

detach(data.r)
```

```{r, echo = FALSE, out.width="60%", fig.align = 'center', warning=FALSE}

# #BIC(reduced.log.cr)
# #AIC(reduced.log.cr)
# data.r$logPrice<-log(price)
# data.r$crcarat<-carat^(1/3)
# # build model
# model <- lm(logPrice~crcarat, data= data.r)
# 
# # Training RMSE ----
# #forecast::accuracy(model) # visually look for RMSE
# 
# test$logPrice<-log(test$price)
# test$crcarat<-test$carat^(1/3)
# Testing RMSE ----
# pred is a vector of prediction
#pred <- predict.lm(model, newdata = test)
#rmse(test$logPrice,pred)

#BIC 19274.77
#AIC 19245.02
#RMSE  0.258073
```
Since the data came sorted by price, there was an obvious correlation between the data. After randomizing the order of the data the ACF plot looks much better. There are still a few places with significant lag, but these are still very small values less than 0.01. The QQ plot shows the residuals follow a normal distribution, meeting our assumptions for a linear model. 
The Final linear model ended at $$log(price)=6.6346 \cdot \sqrt[3]{carat} + 1.8482$$ or $$price = 6.3486 \cdot e^{6.6346*\sqrt[3]{carat}}$$ This model accounts for 93.3% of the variance in the data and gives a great fit for the data for only considering one variable. The 95% interval for the coefficient of the carat (6.6255,6.6436) and (1.8406,1.8559) for the intercept, both of which indicate a high level of certainty in the model. 

## Multiple Linear Regression (with only categorical variables) 
The second model we build uses multiple linear regression along with the categorical variables in the dataset. First, we started fitting with price as the response variable and all the other variables as the predictor variables.

```{r}
data <-
  read.table("clean_diamond_data_train.csv",
             header = TRUE,
             sep = ",")
###################################################

# helper function:plot
assumption_plot <- function(result) {
  # plot
  par(mfrow = c(1, 1))
  ##residual plot of model with no interaction
  plot(result$fitted.values, result$residuals, main = "Residual plot")
  abline(h = 0, col = "red")
  
  ##ACF plot of residuals
  acf(result$residuals, ylim = c(-0.01, 0.01))
  
  ##QQ plot of residuals
  qqnorm(result$residuals)
  qqline(result$residuals, col = "red")
}
```
```{r, echo=FALSE, comment="",results='hide'}
# all categories without log transformation or interactions
result <- lm(data$price ~ ., data = data)
summary(result)
```
Based on the summary of the model specification, we see that the t test suggested that all $\beta$ terms are statistically significant. The F statistic suggested that the the response variable is linearly related with at least one predictor variable. However, the multiple $R^2$ and adjusted $R^2$ are 0.57, which suggested that only 57% of the variation can be explained by the predictor variables. Therefore, one or more assumptions of the multiple linear regression must have been violated. We used the residual plot, ACF plot and QQ plot to check whether any assumptions have been violated: 

```{r, out.width='50%', fig.show = 'hold', results='hide',fig.keep='all', warning=FALSE}
assumption_plot(result)
```

It is clear that the assumptions of mean residual and constant variance and the assumption of normality are not met for this data. The ACF plot shows very big autocorrelations, but we figured it is because the data has been sorted by price. Therefore, we did a random shuffling of the rows for our next model and see if the ACF plot will have better distribution.
\newline

Based on our group exploratory data analysis and findings from SLR model, we chose to eliminate any data that have carat greater than or equal to 2. Log transformation on the response variable and cubic root transformation on "carat"" yielded the best result in a SLR setting. Therefore, we also did the same transformation on the response variable and "carat" to check whether the transformation can further improve the fit.
```{r, echo=FALSE,warning=FALSE,message=FALSE,error=FALSE, results='hide', comment=""}
# randomize data by row
set.seed(42)
rows <- sample(nrow(data))
data.r <- data[rows, ]

# filter out outliers based on carat < 2, and log transformation on y
data.small <- subset(data.r , carat < 2)
data.cr.x <- data.small
data.cr.x$carat <- (data.cr.x$carat) ^ (1 / 3)
lr.cr.x <-  lm(log(data.cr.x$price) ~ ., data = data.cr.x)
summary(lr.cr.x)
```

```{r,out.width='50%', fig.show = 'hold', results='hide',fig.keep='all', warning=FALSE}
assumption_plot(lr.cr.x)
```

The plots showed that the fit is improved further and all the plots, especially the residual plot and the QQ plot, now look very good. Then, we did the same calculations on the evaluation metrics of this new model. The t tests show that all terms are significant so that we don't have to consider dropping any predictor variables. The reference classes for clarity, color and cut are FL, D and Astor Ideal respectively. All slope coefficients are shown to have a p-value smaller than 0.05 so that we want to include every term in the model.
\newline
\newline
The adjusted $R^2$ is 0.9732. The adjusted $R^2$ are better than the previous model. The adjusted $R^2$ is not significantly smaller than the $R^2$, indicating that we probably didn't overfit the model. In conclusion, this model, with log transformation on the response variable and cubic root transformation on the carat variable as well as all the categorical variables, is shown to be the best model so far. The regression equation will be 

$$
\begin{aligned}
log(price) &= 6.830423(\beta_{carat})^{1/3} -0.226630\beta_{clarityIF} -0.573925 \beta_{claritySI1} \\
&- 0.714277\beta_{claritySI2} -0.413591\beta_{clarityVS1} - 0.466890 \beta_{clarityVS2}\\ 
&- 0.301286\beta_{clarityVVS1} - 0.365513 \beta_{clarityVVS2} - 0.063282\beta_{colorE}  \\
&- 0.088755\beta_{colorF} -0.142225\beta_{colorG} - 0.198601\beta_{colorH}\\ 
&- 0.297845\beta_{colorI} -0.420041\beta_{colorJ}-0.284020\beta_{cutGood}\\ &-0.093465\beta_{cutIdeal}-0.246152\beta_{cutVery Good} + 2.436036
\end{aligned}
$$

## Multiple Linear Regression (with interaction terms)

# Conclusion
```{r}
# Create a table of model by forecast accuracy
```
Based on the forecast accuracy table above, we can see that our multiple linear regression model with only categorical variables has the best root mean square error and adjusted $R^2$. Therefore, our final model recommendation for people who wants to estimate the price of a diamond based on its weight, clarity, cut and color will be: 
$$
\begin{aligned}
log(price) &= 6.830423(\beta_{carat})^{1/3} -0.226630\beta_{clarityIF} -0.573925 \beta_{claritySI1} \\
&- 0.714277\beta_{claritySI2} -0.413591\beta_{clarityVS1} - 0.466890 \beta_{clarityVS2}\\ 
&- 0.301286\beta_{clarityVVS1} - 0.365513 \beta_{clarityVVS2} - 0.063282\beta_{colorE}  \\
&- 0.088755\beta_{colorF} -0.142225\beta_{colorG} - 0.198601\beta_{colorH}\\ 
&- 0.297845\beta_{colorI} -0.420041\beta_{colorJ}-0.284020\beta_{cutGood}\\ &-0.093465\beta_{cutIdeal}-0.246152\beta_{cutVery Good} + 2.436036
\end{aligned}
$$



