{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5: Bayesian Graphical Models\n",
    "\n",
    "- Name: Congxin (David) Xu\n",
    "- Computing ID: cx2rx\n",
    "\n",
    "### Honor Pledge: \n",
    "I have neither given nor received aid on this assignment.\n",
    "\n",
    "### Problem 1\n",
    "(30) This problem explores the use of variational approximation in Latent\n",
    "Dirichlet Allocation (LDA). We will use the implementation in sklearn of\n",
    "the variational approximation algorithm in [1].\n",
    "\n",
    "#### Part a\n",
    "\n",
    "Use the notation in the diagram in Figure 1 [2] to write the target\n",
    "posterior distribution of the latent variables and parameters for the\n",
    "general LDA method. Why do we use variational approximation\n",
    "rather than conjugate priors or sampling to obtain this posterior\n",
    "distribution?\n",
    "\n",
    "**Answer**\n",
    "\n",
    "The posterior distribution of the latent variables is \n",
    "$$ p(\\theta, z | w, \\alpha, \\beta) = \\frac{p(\\theta, z, w| \\alpha, \\beta)}{p(w | \\alpha, \\beta)}  $$\n",
    "\n",
    "The reason to use variational approximation rather than conjugate priors or sampling to obtain this posterior distribution is becuase for many models, it is intractable to compute the posterior distribution. Sampling methods are limited to smaller data problems but variational approximation can handle large problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part b\n",
    "\n",
    "Accident reports provide a good use-case for LDA since the narrative\n",
    "information in these reports is frequently overlooked in safety analysis. LDA allows us to capture elements (topics) in this narrative\n",
    "data and use them to better understand unsafe conditions. For this\n",
    "use-case, modify the LDA class for Wikipedia in the `LDA Examples Wikipedia and Trains` jupyter notebook to perform LDA on\n",
    "the accident narratives. About 10 years of these narratives are in\n",
    "the json file, `TrainNarratives.txt`. Use this class to obtain 10\n",
    "topics from the accident narratives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import wikipedia\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "\n",
    "# Set stop words\n",
    "stopWords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDA_trains:\n",
    "    \"\"\"Creates a class for Latent Dirichlet Allocation using summaries from Wikipedia\n",
    "    Input:\n",
    "        reports = list of narratives from accident reports\n",
    "        N_topics = number of topics for LDA to produce\n",
    "        N_words = the number of words to show in a topic\n",
    "        new_report = narrative for a new accident report not in the training set\n",
    "    Methods:\n",
    "        Topics = output the list of topics in the selected narratives\n",
    "        Predict_Topics = Show the predicted probabilities for topics for a new accident narrative\n",
    "            Input: new narrative\n",
    "            \"\"\"\n",
    "    def __init__(self, reports, N_topics=3, N_words = 10):\n",
    "        # the narrative reports\n",
    "        self.reports = reports\n",
    "        # initialize variables\n",
    "        self.N_topics = N_topics\n",
    "        self.N_words = N_words\n",
    "        \n",
    "        # Get the word counts in the reports\n",
    "        self.countVectorizer = CountVectorizer(stop_words='english')\n",
    "        self.termFrequency = self.countVectorizer.fit_transform(self.reports)\n",
    "        self.Words = self.countVectorizer.get_feature_names()\n",
    "        \n",
    "    def Topics(self):\n",
    "                \n",
    "        # Obtain the estimates for the LDA model \n",
    "        self.lda = LatentDirichletAllocation(n_components=self.N_topics)\n",
    "        self.lda.fit(self.termFrequency)\n",
    "        \n",
    "        # Obtain the list of the top N_words in the topics\n",
    "        topics = list()\n",
    "        for topic in self.lda.components_:\n",
    "            topics.append([self.Words[i] for i in topic.argsort()[:-self.N_words - 1:-1]])\n",
    "            \n",
    "        # For each of the topics in the model add the top N_words the list of topics\n",
    "        ### Your code here\n",
    "        # Create column names for the output matrix\n",
    "        cols = list()\n",
    "        for i in range(self.N_words):\n",
    "            cols.append(\"Word \" + (str(i)))\n",
    "            \n",
    "        # Create a dataframe with the topic no. and the words in each topic \n",
    "        # output this dataframe \n",
    "        Topics_df = pd.DataFrame(topics, columns = cols)\n",
    "        Topics_df.index.name = \"Topics\"\n",
    "        return Topics_df\n",
    "    \n",
    "    def Predict_Topics(self, new_reports):\n",
    "        self.new_reports = new_reports\n",
    "        \n",
    "        # Get the list of new accident report narratives\n",
    "        # and the number of new narratives\n",
    "        N_new_reports = len(self.new_reports)\n",
    "        \n",
    "        \n",
    "        # For each of the new narratives \n",
    "        # obtain the estimated probabilities for each of the topics\n",
    "        # in each of the new narratives as estimated by the LDA results\n",
    "        # on the training set \n",
    "        new_report_topics = list()\n",
    "        ### Your code here        \n",
    "        for i in self.new_reports:\n",
    "            new_report_topics.append(self.lda.\\\n",
    "                                     transform(self.countVectorizer.\\\n",
    "                                               transform([i])))\n",
    "        \n",
    "        # Recast the list of probabilities for topics as an array \n",
    "        # of size no. of new reports X no. of topics\n",
    "        new_report_topics = np.array(new_report_topics).\\\n",
    "            reshape(N_new_reports, self.N_topics)\n",
    "        \n",
    "        # Create column names for the output dataframe\n",
    "        cols = list()\n",
    "        ### Your code here        \n",
    "        for i in range(self.N_topics):\n",
    "            cols.append(\"Topic \"+(str(i)))\n",
    "            \n",
    "        # Create the dataframe whose rows contain topic probabilities for \n",
    "        # specificed narratives/reports\n",
    "        ### Your code here\n",
    "        New_Reports_df = pd.DataFrame(new_report_topics, columns = cols)        \n",
    "        New_Reports_df.insert(0, 'Reports', self.new_reports)\n",
    "        \n",
    "        return New_Reports_df\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['UNITS 231-281(BACK TO BACK)  WERE COMING INTO UP DEISEL SHOP  WHEN THE LEFT WHEEL OF 281 RODE OVER RECENTLY REPAIRED SWITCH PLATE AND DERAILED. THE CAUSE WAS DETERMINED TO BE THE TRACK TELEMETRY IN THAT IT WAS TOO SHARP OF A CURVE.',\n",
       " 'ENGINE 286 CAUGHT FIRE AT THE SPRINGFIELD, MA STATION DUE TO BEARINGS IN MAIN GENERATOR LET GO.',\n",
       " 'TRAIN NO.#4 WITH ENGS 83/11/90/44 AND 11 CARS DERAILED 2 DEADHEAD CARS, C/44834 AND C/9639, WHILE MAKING A SHOVING MOVE ONTO TRACK 28.  THE DERAILMENT WAS DUE TO HIGH BUFF FORCES CAUSED JACKKNIFING OFDEADHEADING AMFLEET CAR 44834 LOCATED DIRECTLY BEHIND ENGINES DUE TO EXCESSIVE AMPERAGE GENERATED BY FOUR P42 LOCOMOTIVES SHOVING TRAIN AGAINST AN APPROXIMATELY 15-POUND BRAKE REDUCTION.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train accident narratives are in a json file\n",
    "# Read the JSON file with the narratives and convert to a list for the LDA analysis\n",
    "\n",
    "with open('TrainNarratives.txt') as json_file:  \n",
    "    Narrative_dict = json.load(json_file)\n",
    "    \n",
    "train_reports = list(Narrative_dict.values())\n",
    "    \n",
    "train_reports[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word 0</th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>Word 3</th>\n",
       "      <th>Word 4</th>\n",
       "      <th>Word 5</th>\n",
       "      <th>Word 6</th>\n",
       "      <th>Word 7</th>\n",
       "      <th>Word 8</th>\n",
       "      <th>Word 9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topics</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>car</td>\n",
       "      <td>switch</td>\n",
       "      <td>cars</td>\n",
       "      <td>derailed</td>\n",
       "      <td>track</td>\n",
       "      <td>rail</td>\n",
       "      <td>point</td>\n",
       "      <td>end</td>\n",
       "      <td>west</td>\n",
       "      <td>shoving</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>damage</td>\n",
       "      <td>equipment</td>\n",
       "      <td>track</td>\n",
       "      <td>vehicle</td>\n",
       "      <td>machine</td>\n",
       "      <td>struck</td>\n",
       "      <td>00</td>\n",
       "      <td>ballast</td>\n",
       "      <td>estimated</td>\n",
       "      <td>operator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train</td>\n",
       "      <td>cars</td>\n",
       "      <td>derailed</td>\n",
       "      <td>car</td>\n",
       "      <td>track</td>\n",
       "      <td>crew</td>\n",
       "      <td>emergency</td>\n",
       "      <td>went</td>\n",
       "      <td>rail</td>\n",
       "      <td>main</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>derailed</td>\n",
       "      <td>cars</td>\n",
       "      <td>rail</td>\n",
       "      <td>track</td>\n",
       "      <td>pulling</td>\n",
       "      <td>broken</td>\n",
       "      <td>wide</td>\n",
       "      <td>train</td>\n",
       "      <td>yard</td>\n",
       "      <td>gauge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>car</td>\n",
       "      <td>fuel</td>\n",
       "      <td>gallons</td>\n",
       "      <td>cars</td>\n",
       "      <td>train</td>\n",
       "      <td>end</td>\n",
       "      <td>derailed</td>\n",
       "      <td>released</td>\n",
       "      <td>journal</td>\n",
       "      <td>tank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>track</td>\n",
       "      <td>cars</td>\n",
       "      <td>car</td>\n",
       "      <td>lead</td>\n",
       "      <td>cut</td>\n",
       "      <td>crew</td>\n",
       "      <td>end</td>\n",
       "      <td>switch</td>\n",
       "      <td>yard</td>\n",
       "      <td>derailed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>damage</td>\n",
       "      <td>pantograph</td>\n",
       "      <td>train</td>\n",
       "      <td>car</td>\n",
       "      <td>causing</td>\n",
       "      <td>track</td>\n",
       "      <td>struck</td>\n",
       "      <td>wire</td>\n",
       "      <td>catenary</td>\n",
       "      <td>engine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hazardous</td>\n",
       "      <td>materials</td>\n",
       "      <td>released</td>\n",
       "      <td>derailed</td>\n",
       "      <td>cars</td>\n",
       "      <td>track</td>\n",
       "      <td>railcars</td>\n",
       "      <td>loads</td>\n",
       "      <td>pulling</td>\n",
       "      <td>yard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>conductor</td>\n",
       "      <td>engineer</td>\n",
       "      <td>cars</td>\n",
       "      <td>car</td>\n",
       "      <td>stop</td>\n",
       "      <td>track</td>\n",
       "      <td>movement</td>\n",
       "      <td>train</td>\n",
       "      <td>shoving</td>\n",
       "      <td>crew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>switch</td>\n",
       "      <td>lined</td>\n",
       "      <td>train</td>\n",
       "      <td>movement</td>\n",
       "      <td>ran</td>\n",
       "      <td>crew</td>\n",
       "      <td>failed</td>\n",
       "      <td>signal</td>\n",
       "      <td>line</td>\n",
       "      <td>main</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Word 0      Word 1    Word 2    Word 3   Word 4  Word 5     Word 6  \\\n",
       "Topics                                                                          \n",
       "0             car      switch      cars  derailed    track    rail      point   \n",
       "1          damage   equipment     track   vehicle  machine  struck         00   \n",
       "2           train        cars  derailed       car    track    crew  emergency   \n",
       "3        derailed        cars      rail     track  pulling  broken       wide   \n",
       "4             car        fuel   gallons      cars    train     end   derailed   \n",
       "5           track        cars       car      lead      cut    crew        end   \n",
       "6          damage  pantograph     train       car  causing   track     struck   \n",
       "7       hazardous   materials  released  derailed     cars   track   railcars   \n",
       "8       conductor    engineer      cars       car     stop   track   movement   \n",
       "9          switch       lined     train  movement      ran    crew     failed   \n",
       "\n",
       "          Word 7     Word 8    Word 9  \n",
       "Topics                                 \n",
       "0            end       west   shoving  \n",
       "1        ballast  estimated  operator  \n",
       "2           went       rail      main  \n",
       "3          train       yard     gauge  \n",
       "4       released    journal      tank  \n",
       "5         switch       yard  derailed  \n",
       "6           wire   catenary    engine  \n",
       "7          loads    pulling      yard  \n",
       "8          train    shoving      crew  \n",
       "9         signal       line      main  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_train = LDA_trains(reports = train_reports, N_topics = 10, N_words = 10)\n",
    "lda_train.Topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part c\n",
    "\n",
    "Use the class you developed for Problem 1b to obtain the probabilities\n",
    "for each of the topics in the first 10 narratives in the `TrainNarratives.txt`\n",
    "data set. What is the notation in Figure1 that represents these prob-\n",
    "abilities?\n",
    "\n",
    "**Answer**\n",
    "\n",
    "The notation in Figure 1 that represents these probabilities is $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reports</th>\n",
       "      <th>Topic 0</th>\n",
       "      <th>Topic 1</th>\n",
       "      <th>Topic 2</th>\n",
       "      <th>Topic 3</th>\n",
       "      <th>Topic 4</th>\n",
       "      <th>Topic 5</th>\n",
       "      <th>Topic 6</th>\n",
       "      <th>Topic 7</th>\n",
       "      <th>Topic 8</th>\n",
       "      <th>Topic 9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UNITS 231-281(BACK TO BACK)  WERE COMING INTO ...</td>\n",
       "      <td>0.319121</td>\n",
       "      <td>0.004546</td>\n",
       "      <td>0.324309</td>\n",
       "      <td>0.004546</td>\n",
       "      <td>0.004546</td>\n",
       "      <td>0.004546</td>\n",
       "      <td>0.324746</td>\n",
       "      <td>0.004547</td>\n",
       "      <td>0.004546</td>\n",
       "      <td>0.004546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENGINE 286 CAUGHT FIRE AT THE SPRINGFIELD, MA ...</td>\n",
       "      <td>0.009095</td>\n",
       "      <td>0.009098</td>\n",
       "      <td>0.009098</td>\n",
       "      <td>0.009094</td>\n",
       "      <td>0.009095</td>\n",
       "      <td>0.009094</td>\n",
       "      <td>0.918137</td>\n",
       "      <td>0.009095</td>\n",
       "      <td>0.009093</td>\n",
       "      <td>0.009102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN NO.#4 WITH ENGS 83/11/90/44 AND 11 CARS ...</td>\n",
       "      <td>0.574898</td>\n",
       "      <td>0.002326</td>\n",
       "      <td>0.272445</td>\n",
       "      <td>0.002326</td>\n",
       "      <td>0.002326</td>\n",
       "      <td>0.002326</td>\n",
       "      <td>0.002326</td>\n",
       "      <td>0.136375</td>\n",
       "      <td>0.002326</td>\n",
       "      <td>0.002326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Reports   Topic 0   Topic 1  \\\n",
       "0  UNITS 231-281(BACK TO BACK)  WERE COMING INTO ...  0.319121  0.004546   \n",
       "1  ENGINE 286 CAUGHT FIRE AT THE SPRINGFIELD, MA ...  0.009095  0.009098   \n",
       "2  TRAIN NO.#4 WITH ENGS 83/11/90/44 AND 11 CARS ...  0.574898  0.002326   \n",
       "\n",
       "    Topic 2   Topic 3   Topic 4   Topic 5   Topic 6   Topic 7   Topic 8  \\\n",
       "0  0.324309  0.004546  0.004546  0.004546  0.324746  0.004547  0.004546   \n",
       "1  0.009098  0.009094  0.009095  0.009094  0.918137  0.009095  0.009093   \n",
       "2  0.272445  0.002326  0.002326  0.002326  0.002326  0.136375  0.002326   \n",
       "\n",
       "    Topic 9  \n",
       "0  0.004546  \n",
       "1  0.009102  \n",
       "2  0.002326  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_train.Predict_Topics(train_reports[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part d\n",
    "\n",
    "Briefly explain how a safety engineer at Federal Railroad Administration could use the results you obtain in Problem 1c to improve safety for trains.\n",
    "\n",
    "**Answer**\n",
    "\n",
    "The safety engineer at Federal Rialroad Administration could look at the train report and the topic that report belongs to. Then, the safety engineer will be able to review the words under that topic and pay special attention to those areas when he/she conduct the safety checks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "(10) Neural network are a graphical model, Markov Networks, that can\n",
    "be analyzed with Bayesian methods and Boltzmann machines are examples. Provide an explanation for the intractability of the calculation of the\n",
    "partition function, Z, in a Boltzmann machine. Explain how restricted Boltzmann machines help with this problem. Also explain how MCMC\n",
    "and variational methods can provide approximations to Z in restricted\n",
    "Boltzman machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3\n",
    "\n",
    "(30) The following questions are based on reading and running the jupyter\n",
    "notebook, `pymc3-variation-inference-neural-network.ipynb`,\n",
    "by Thomas Wiecki, updated by Maxim Kochurov as provided in their blog\n",
    "post. Run the notebook and then answer these questions.\n",
    "\n",
    "####  Part a\n",
    "\n",
    "Wieki says that an advantage to using Bayesian modeling with neural\n",
    "network and deep learning is that \"we could train the model specifically on samples it is most uncertain about.\" Explain how he finds\n",
    "these samples in this example. Explain how you would implement\n",
    "his suggestion (you do not have to actually implement this).\n",
    "\n",
    "**Answer**\n",
    "\n",
    "Thomas Wiecki calculates the standard deviation of the posterior predictive to get a sense for the uncertainty in his predictions. He also created a heapmap based on the standard deviation of each posterior prediction to get a sense of where the predictions surface is with high standard deviations.  \n",
    "\n",
    "With the standard deviation calculated, I would separated the data with highest uncertainty out, for example top 25% of the data with highest standard deviation on posterior prediction. Then, I would build a new model on those 25% of the data. Once I have the new model, I will refit the entire dataset (100%) using the new model to get a new posterior prediction. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part b\n",
    "\n",
    "Wieki also says that another advantage to Bayesian modeling with\n",
    "neural network and deep learning is that \\We also get uncertainty\n",
    "estimates of our weights which could inform us about the stability\n",
    "of the learned representations of the network.\" Discuss what the\n",
    "uncertainty estimates for the weights found for the example in this\n",
    "notebook imply.\n",
    "\n",
    "**Answer**\n",
    "\n",
    "Different layers of the neural network has different weights for the same predictor. We can view the uncertainty estimates for the weights same as the way we viewed the confidence interval for regression coefficients. If the uncertainty estimates for the weights covers the value of 0, that implies that such weights may not be useful in our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part c\n",
    "\n",
    "Explain how the Gaussian priors help to regularize the weights in the\n",
    "neural network.\n",
    "\n",
    "**Answer**\n",
    "\n",
    "Since we are using MAP estimation for the posterior distributionm, we are trying to maximize the probability of the parameter given the data: \n",
    "\n",
    "$$P(W|x, y) = \\frac{P(x, y | W) P(W)}{P(x ,y)} $$\n",
    "\n",
    "where $W$ is the weights of predictor $x$, and $y$ is the response variable.\n",
    "\n",
    "Since $P(x, y)$ is the normalizing constant, we can ignore that part. The maximum log likelihood then become \n",
    "\n",
    "$$log(P(W|x, y)) = log(P(x, y | W)) +  log(P(W))$$\n",
    "\n",
    "If we assume the Gaussian priors, each element of the weights are drawn independently from a unit Gaussian, then \n",
    "\n",
    "$$log(P(W)) = log( \\frac{1}{\\sqrt{2\\pi}} \\prod_{ij} exp(- \\frac{w_{ij}^2}{2} )) = -\\frac{1}{2} \\sum_{ij} w_{ij}^2 +  log( \\frac{1}{\\sqrt{2\\pi}}) $$\n",
    "\n",
    "Now the log likelihood becomes \n",
    "\n",
    "$$log(P(W|x, y)) = log(P(x, y | W)) - \\frac{1}{2} \\sum_{ij} w_{ij}^2 + c$$\n",
    "where $c = log( \\frac{1}{\\sqrt{2\\pi}})$, which is a constant.\n",
    "\n",
    "Therefore, the regularitzation penalty $R(W) = \\frac{1}{2} \\sum_{ij} w_{ij}^2$.\n",
    "\n",
    "\n",
    "Reference: https://stats.stackexchange.com/questions/229415/why-is-regularization-interpreted-as-a-gaussian-prior-on-my-weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part d\n",
    "\n",
    "Why do we use a variational apprxomiation instead of sampling for estimating the posterior of the weights? \n",
    "\n",
    "**Answer**\n",
    "\n",
    "Because the sampling method cannot scale to very large data sets and it can be very slow when working with high dimensional parameters or latent space. The posterior distribution may also be very complex to draw samples from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part e\n",
    "\n",
    "Change the prior distributinos for all three sets of the neural net weights to Cauchy with location (alpha) = 0 and scale (beta) = 2. Rerun the remaining cells in the notebook and comment on any changes you see from this. \n",
    "\n",
    "**Answer**\n",
    "- The Average Loss coming out of the ADVI model is higher.\n",
    "- The toal prediction accuracy becomes better, changing from 95% to 96.4%\n",
    "- The final Minibatch-ADVI model now have 3 weights that does not contain 0 in its corresponding distribution. Previously, there were only 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4\n",
    "\n",
    "(30) You are tracking the performance of a set of companies with the idea\n",
    "that you might possibly buy stock in them. You decide to automate this process using HMM and you implement your first version for one company.\n",
    "This company has three states that are hidden from investors: (1) in-\n",
    "trouble; (2) static; and (3) major growth potential. You have estimated\n",
    "the transition probabilities between states as follows:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    ".6 & .3 & .1\\\\ \n",
    ".4 & .4 & .2\\\\ \n",
    ".1 & .4 & .5\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "You have a text analysis system using Naive Bayes to process the quarterly\n",
    "reports and assess their sentiment into one of three categories: (1) Fine;\n",
    "(2) Good; and (3) Very good. Your estimates for the probabilities of these\n",
    "sentiments given the state of the company are shown in the following\n",
    "matrix (the sentiments are in the rows and the states of the company are\n",
    "in the columns).\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    ".45 & .4 & .15\\\\ \n",
    ".3 & .4 & .3\\\\ \n",
    ".2 & .5 & .3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "You have 3 quarterly reports with the assessments: Fine, Fine, Very Good\n",
    "and your prior for the initial state is equally likely for each value. The\n",
    "following questions use the HMM class in the jupyter notebook, HMM\n",
    "Examples HW5 - Burglary and Investment with your additions\n",
    "to it as indicated in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Nov 12 20:35:27 2020\n",
    "\n",
    "@author: donaldbrown\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class HMM:\n",
    "    \"\"\"Creates a class for Hidden Markov Models\n",
    "    Input:\n",
    "        Viz:      List of observed or visible states over time\n",
    "        Trans_M:  Transition matrix for hidden states, H X H, H=len(Trans_M), no. of hidden variables\n",
    "        Obs_M:    Observation matrix, H X V, V = no. of visible variables\n",
    "        Pi:       List of initial state probabilities\n",
    "    Methods:\n",
    "        filter = The posterior probabilities for hidden states for each time period, T X H array\n",
    "        smoother = The probabiliteis for the hidden states at each prevoius time period, T X H array\n",
    "        viturbi = The most likely path of hidden states given the observed state, data frame, 1 X T\n",
    "        predictor = The probabilities for next hidden state and the next observed state, 1 X H array \"\"\"\n",
    "    \n",
    "    def __init__(self,Viz, Trans_M, Obs_M, Pi):\n",
    "        # initialize variables\n",
    "        # Hidden state transition matrix\n",
    "        self.Trans_M = Trans_M\n",
    "        # Visible or observates state probabilities given the hidden states\n",
    "        self.Obs_M = Obs_M\n",
    "        # No. of hidden states\n",
    "        self.H = Trans_M.shape[0]\n",
    "        # No. of observed states\n",
    "        self.V = Obs_M.shape[0]\n",
    "        # prior probabaiities for the hidden states\n",
    "        self.Pi = Pi\n",
    "        # List of observed states over time\n",
    "        self.Viz = Viz\n",
    "\n",
    "        \n",
    "    def filter(self):\n",
    "        \n",
    "        T = len(self.Viz)\n",
    "        \n",
    "        # Obtain the joint probabilities of the hidden and observed states at time t\n",
    "        self.alpha = np.zeros((T, self.H))\n",
    "        self.alpha[0, :] = self.Pi * self.Obs_M[:,self.Viz[0]]\n",
    " \n",
    "        for t in range(1, T):\n",
    "            for j in range(self.H):\n",
    "                self.alpha[t, j] = self.alpha[t - 1].dot(self.Trans_M[:, j]) * self.Obs_M[j, self.Viz[t]]\n",
    "        \n",
    "        ### Insert your code here to computer the posterior probabilities ###\n",
    "        self.Post = np.zeros((T, self.H))\n",
    "        \n",
    "        for t in range(0, T):\n",
    "            sum_of_row = sum(self.alpha[t,:])\n",
    "            for j in range(self.H):\n",
    "                self.Post[t, j] = self.alpha[t, j] / sum_of_row\n",
    "\n",
    "        print(\"self.alpha\")\n",
    "        print(self.alpha)\n",
    "        print(\"Posterior\")\n",
    "        print(self.Post)   \n",
    "        return self.Post\n",
    "      \n",
    "    def smoother(self):\n",
    "\n",
    "        T = len(self.Viz)\n",
    "        self.beta = np.zeros((T, self.H))\n",
    " \n",
    "        # setting beta(T) = 1\n",
    "        self.beta[T - 1] = np.ones((self.H))\n",
    " \n",
    "        # Loop backwards way from T-1 to 1\n",
    "        # Due to python indexing the actual loop will be T-2 to 0\n",
    "        for t in range(T - 2, -1, -1):\n",
    "            for j in range(self.H):\n",
    "                self.beta[t, j] = (self.beta[t + 1] * self.Obs_M[:, self.Viz[t + 1]]).dot(self.Trans_M[j, :])\n",
    "                \n",
    "        # Obtain the posterior probabilities of the hidden states given the observed states       \n",
    "        \n",
    "        ### Insert your code here to compute the posterior probabilities ###    \n",
    "        self.Post_smoother = np.zeros((T, self.H))\n",
    "        \n",
    "        for t in range(0, T):\n",
    "            sum_of_row = sum(self.alpha[t] * self.beta[t])\n",
    "            \n",
    "            for j in range(self.H):\n",
    "                self.Post_smoother[t, j] = (self.alpha[t, j] * self.beta[t, j])  / sum_of_row      \n",
    "        \n",
    "        print(\"beta\")\n",
    "        print(self.beta)\n",
    "        print(\"Posterior\")\n",
    "        print(self.Post_smoother)\n",
    " \n",
    "        return self.Post_smoother\n",
    "    \n",
    "    \n",
    "    def viturbi(self):\n",
    "        T = len(self.Viz)\n",
    "        \n",
    "        # Obtain the joint probabity of the most likely path that ends in state j at time t\n",
    "        delta = np.zeros((T, self.H))\n",
    "        delta[0, :] = (self.Pi * self.Obs_M[:, Viz[0]])\n",
    " \n",
    "\n",
    "        prev = np.zeros((T, self.H))\n",
    "        prev[0,:] = np.repeat(None, 3)\n",
    " \n",
    "        for t in range(1, T):\n",
    "            for j in range(self.H):\n",
    "                # The most likely state given our previoius state at t-1\n",
    "                \n",
    "                prob = delta[t - 1] * (self.Trans_M[:, j])\n",
    " \n",
    "                #  The probability of the most probable state given the previous state and the observation at time t\n",
    "                \n",
    "                delta[t, j] = np.max(prob) * (self.Obs_M[j, Viz[t]])                \n",
    "                \n",
    "                # The most probable state given previous state \n",
    "\n",
    "                prev[t, j] = np.argmax(prob)\n",
    " \n",
    "                \n",
    "        # Path Array\n",
    "        S = np.zeros(T)\n",
    " \n",
    "        # Find the most probable last hidden state\n",
    "        last_state = np.argmax(delta[T-1, :])\n",
    " \n",
    "        S[T-1] = last_state\n",
    "        \n",
    "        # Find the most probable hidden states at the previous times\n",
    "        # Walk backwords\n",
    "        ### Insert your code here ###\n",
    "        for t in range(T-1, 0, -1):\n",
    "            S[t] = np.argmax(delta[t, :])\n",
    "            \n",
    "        # Change to states numbers in problem (i.e., +1)\n",
    "        S = S+1\n",
    "            \n",
    "        S = S.reshape([1,3])\n",
    " \n",
    "        # Path, S, as a dataframe \n",
    "        # Create a list of column names, Time  \n",
    "        cols = list()\n",
    "        for i in range(1,T+1):\n",
    "            cols.append(\"Time \"+(str(i)))\n",
    "        Path = pd.DataFrame(S, columns = cols)\n",
    "        print('delta')\n",
    "        print(delta)\n",
    "        print('Previous')\n",
    "        print(prev)        \n",
    "        print(\"Path\")\n",
    "        print(Path)\n",
    "        return Path\n",
    " \n",
    "\n",
    "    def predictor(self, steps = 1):\n",
    "        T = len(self.Viz)\n",
    "        # Hidden state prediction probabilities using filtering results (Post)\n",
    "        ### Insert your code here ### dot product of filter post and transition matrix\n",
    "        Pred_Hidden = Pred_Hidden = self.Post[T-1, :] @ self.Trans_M \n",
    "        print(\"Predicted Hidden State\")\n",
    "        print(Pred_Hidden)\n",
    "        # Visible state prediction using the predicted hidden state probabilities\n",
    "        ### Insert your code here ### dot product pred_hidden with observed matrix\n",
    "        Pred_Visible = Pred_Visible = Pred_Hidden @ self.Obs_M\n",
    "        print(\"Predicted Visible State\")\n",
    "        print(Pred_Visible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transition matrix\n",
    "TM = np.array([[.6,.3,.1],[.4,.4,.2],[.1,.4,.5]])  \n",
    "# Observation matrix\n",
    "OM = np.array([[.45,.4,.15],[.3,.4,.3],[.2,.5,.3]])\n",
    "OM = OM.T\n",
    "# # Prior probabilities of hidden states\n",
    "p = [1/3, 1/3, 1/3]\n",
    "# # Observed visible states\n",
    "Viz = [0, 0, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part a \n",
    "In order to decide whether to invest, find the most likely current state\n",
    "given the observed states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.alpha\n",
      "[[0.15       0.13333333 0.05      ]\n",
      " [0.06675    0.04733333 0.01      ]\n",
      " [0.01199667 0.02147917 0.0063425 ]]\n",
      "Posterior\n",
      "[[0.45       0.4        0.15      ]\n",
      " [0.53794493 0.38146407 0.080591  ]\n",
      " [0.301285   0.53942907 0.15928592]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.45      , 0.4       , 0.15      ],\n",
       "       [0.53794493, 0.38146407, 0.080591  ],\n",
       "       [0.301285  , 0.53942907, 0.15928592]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use Filter to infer the present: get the current state\n",
    "hmm1 = HMM(Viz, TM, OM, p)\n",
    "hmm1.filter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most likely current state given the observed states is **Fine, Fine, Good**\n",
    "\n",
    "#### Part b\n",
    "\n",
    "(b) Using smoothing to find the most likely state at each previous time\n",
    "period (i.e., periods 1 and 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta\n",
      "[[0.12735 0.1195  0.09565]\n",
      " [0.3     0.34    0.37   ]\n",
      " [1.      1.      1.     ]]\n",
      "Posterior\n",
      "[[0.47974133 0.40015068 0.12010799]\n",
      " [0.50290905 0.40416893 0.09292202]\n",
      " [0.301285   0.53942907 0.15928592]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.47974133, 0.40015068, 0.12010799],\n",
       "       [0.50290905, 0.40416893, 0.09292202],\n",
       "       [0.301285  , 0.53942907, 0.15928592]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm1.smoother()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most likely state at time period 1 and 2 is **Fine, Good**\n",
    "\n",
    "#### Part c\n",
    "\n",
    "Show the most likely path of performance through the hidden states\n",
    "up to the current time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta\n",
      "[[0.15       0.13333333 0.05      ]\n",
      " [0.0405     0.02133333 0.004     ]\n",
      " [0.00486    0.006075   0.00128   ]]\n",
      "Previous\n",
      "[[nan nan nan]\n",
      " [ 0.  1.  1.]\n",
      " [ 0.  0.  1.]]\n",
      "Path\n",
      "   Time 1  Time 2  Time 3\n",
      "0     1.0     1.0     2.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time 1</th>\n",
       "      <th>Time 2</th>\n",
       "      <th>Time 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time 1  Time 2  Time 3\n",
       "0     1.0     1.0     2.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm1.viturbi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most likely path of perofrmance through the hidden states up to the current time is **Good, Good, Very Good**\n",
    "\n",
    "#### Part d\n",
    "\n",
    "Find the most ikely hidden state and visible state for this company\n",
    "in the next time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Hidden State\n",
      "[0.41247122 0.3698715  0.21765728]\n",
      "Predicted Visible State\n",
      "[0.36620924 0.33698715 0.33272718]\n"
     ]
    }
   ],
   "source": [
    "hmm1.predictor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The most ikely hidden state for this company in the next time period is **Fine**.\n",
    "- The most ikely visible state for this company in the next time period is also **Fine**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
