{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1; Probability Review and Prior\n",
    "- Name: Congxin (David) \n",
    "- Computer ID: cx2rx\n",
    "\n",
    "### Question 1\n",
    "(20) You are a data science and are choosing between three approaches, A, B, and C, to a problem. With approach A, you will spend a total of **four** days coding and running an algorithm and it will not produce useful results. With approach B, you will spend a total of **three** days coding and running an algorithm and it will not produce useful results. With approach C, you will spend **one** day coding and running an algorithm and it will give the results you are looking for. You are equally likely to choose among unselected options. What is the expected time in days for you to obtain the results you are looking for? What is the variance on this time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "Based on the information provided to us, there are total of 5 possible situations: \n",
    "\n",
    "|   x      |   C\t|   BC\t        |   AC\t        | ABC\t        | BAC              |\n",
    "|  --- \t   |  --- \t|   ---      \t|   ---\t        | ---           | ---              |\n",
    "| P(X = x) |   1/3\t|   1/6\t        |   1/6  \t    | 1/6        \t| 1/6              |\n",
    "\n",
    "Therefore, \n",
    "$$ \\begin{aligned}\n",
    "\\mu & = \\sum_{i = 0}^{5} x_i P (X = x_i) \\\\\n",
    "& = 1 \\times 1/3 + 4 \\times 1/6 + 5 \\times 1/6 + 8 \\times 1/6 + 8 \\times 1/6 \\\\\n",
    "& = 1/3 + 4/6 + 5/6 + 8/6 + 8/6 \\\\\n",
    "& = 4.5\n",
    "\\end{aligned} $$\n",
    "\n",
    "The expected time in days for me to obtian the results I am looking for will be **4.5** days. \n",
    "\n",
    "$$ \\begin{aligned} \n",
    "Var(X) &= \\sum_{i = 0}^{5} (x_i - \\mu)^2 p(X = x_i) \\\\\n",
    "& = (1 - 4.5)^2 \\times 1/3 + (4 - 4.5)^2 \\times 1/6 + (5 - 4.5)^2 \\times 1/6 + (8 - 4.5)^2 \\times 1/6 + (8 - 4.5)^2 \\times 1/6 \\\\\n",
    "& = 8.25\n",
    "\\end{aligned} $$\n",
    "\n",
    "The variance on this time is **8.25**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "(20) Suppose if it is sunny or not in Charlottesville depends on the weather\n",
    "of the last three days. Show how this can be modeled as a Markov chain.\n",
    "\n",
    "**Answer**\n",
    "Given the weather is sunny or not in Charlottesville depends on the weather of the last three days. Let S denote Sunny and N denote not sunny. We can get the following trasition matrix: \n",
    "\n",
    "|   Last Three Days\t|   Sunny (S)\t|   Not Sunny (N)\t| \n",
    "|:-\t                | ---\t        |---\t            |\n",
    "|   \t    SSS     |   P(SSSS)     |   P(SSSN)\t        |\n",
    "|   \t    NSS     |   P(NSSS)\t    |   P(NSSN)\t        |\n",
    "|           SNS    \t|   P(SNSS)\t    |   P(SNSN)\t        |\n",
    "|           SNS    \t|   P(SNSS)\t    |   P(SNSN)\t        |\n",
    "|           NNS    \t|   P(NNSS)\t    |   P(NNSN)\t        |\n",
    "|           NSN    \t|   P(NSNS)\t    |   P(NSNN)\t        |\n",
    "|           SNN    \t|   P(SNNS)\t    |   P(SNNN)\t        |\n",
    "|           NNN    \t|   P(NNNS)\t    |   P(NNNN)\t        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "(20) Assume a Gaussian distribution for observations, $X_i, i = 1, . . . , N$\n",
    "with unknown mean, M, and known variance 5. Suppose the prior for M\n",
    "is Gaussian with variance 10. How large a random sample must be taken\n",
    "(i.e., what is the minimum value for N) to specify an interval having unit\n",
    "length of 1 such that the probability that M lies in this interval is 0.95?\n",
    "\n",
    "**Answer**\n",
    "\n",
    "Give M has a Gaussian distribution and its prior also has an Gaussian distribution. Given $\\sigma^2 = 5$ and $\\sigma^2_0 = 10$, the variance of the posterior distribution is: \n",
    "\n",
    "$$\\sigma^2_{post} = \\frac{\\sigma_0^2 \\sigma^2}{\\sigma^2 + N \\sigma_0^2} = \\frac{10 \\times 5}{5 + 10N} = \\frac{10}{1 + 2N}$$\n",
    "\n",
    "We also know that the 95% confidence interval has a unit lenght of 1 and a z-score of 1.959964. \n",
    "\n",
    "Therefore, we can transform the formula of z-score \n",
    "\n",
    "$$Z = \\frac{x - \\mu}{\\sigma_{post}} \\Rightarrow  1.959964 = \\frac{0.5}{\\sigma_{post}} \\Rightarrow \\sigma_{post} = \\frac{0.5}{1.959964} = 0.25510672645007765$$\n",
    "\n",
    "Substituting the value of $\\sigma_{post}$ to the first equation:\n",
    "\n",
    "$$\\sigma^2_{post} = \\frac{10}{1 + 2N} = 0.25510672645007765^2 $$\n",
    "$$ \\frac{10}{1 + 2N} = 0.260317767520299 \\Rightarrow 1 +2N = 153.65835525184 \\Rightarrow N = 76.32917762592$$\n",
    "\n",
    "Since N must be an integer, we round up the value of N and conclude that we need at least 77 random samples to specify an interval having unit length of 1 such that the probability that M lies in this interval is 0.95."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "(20) You have started an online business selling books that are of interest to your customers. A publisher has just given you a large book with photos from famous 20th century photographers. You think this book will appeal to people who have bought art books, history books, and coffee table books. In an initial offering of the new book, you collect data on purchases of the new\n",
    "book and combine these data with data from the past purchases (see ArtHistBooks.csv). Use Bayesian analysis to give the posterior probabilities for purchases of art books, history books, and coffee table books, as well as, the separate probabilities for purchases of the new book, given each possible combination of prior purchases of art books, history books, and coffee table books. Do this by first using beta priors with values of the hyperparameters that represent lack of prior information. Then compute these probabilities again with beta priors that show strong weighting for low likelihood of a book purchase. Compare your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let 0 denote the event of not purchase any books and 1 denote the event of purchase any books. To represent lack of prior information, we use $Beta(1, 1)$ as the prior distribution. Let A denote the art book, H denote the history books, C denote coffee table books and N denote New books. By counting the non-zero values in each column of the `ArtHistBooks.csv` file, we get: \n",
    "\n",
    "With uninformed prior $Beta(1,1)$: \n",
    "\n",
    "$E(\\theta_{A} | x) =  \\frac{\\sum_{i = 1}^N x_i + \\alpha}{N + \\alpha + \\beta} = \\frac{301 + 1}{1000 + 1 + 1} = 0.3014$\n",
    "\n",
    "$E(\\theta_{H} | x) =  \\frac{\\sum_{i = 1}^N x_i + \\alpha}{N + \\alpha + \\beta} = \\frac{543 + 1}{1000 + 1 + 1} = 0.5429$\n",
    "\n",
    "$E(\\theta_{C} | x) =  \\frac{\\sum_{i = 1}^N x_i + \\alpha}{N + \\alpha + \\beta} = \\frac{380 + 1}{1000 + 1 + 1} = 0.3802$\n",
    "\n",
    "$E(N = 1 | (A = 0, H = 0, C = 0)) = \\frac{\\sum_{i = 1}^N x_i + \\alpha}{N + \\alpha + \\beta} = \\frac{4 + 1}{193 + 1 + 1} = 0.0256$ \n",
    "\n",
    "$E(N = 1 | (A = 1, H = 0, C = 0)) = \\frac{\\sum_{i = 1}^N x_i + \\alpha}{N + \\alpha + \\beta} = \\frac{12 + 1}{76 + 1 + 1} = 0.1667$ \n",
    "\n",
    "$E(N = 1 | (A = 0, H = 1, C = 0)) = \\frac{\\sum_{i = 1}^N x_i + \\alpha}{N + \\alpha + \\beta} = \\frac{10 + 1}{251 + 1 + 1} = 0.0435$ \n",
    "\n",
    "$E(N = 1 | (A = 0, H = 0, C = 1)) = \\frac{\\sum_{i = 1}^N x_i + \\alpha}{N + \\alpha + \\beta} = \\frac{3 + 1}{134 + 1 + 1} = 0.0294$ \n",
    "\n",
    "$E(N = 1 | (A = 1, H = 1, C = 0)) = \\frac{\\sum_{i = 1}^N x_i + \\alpha}{N + \\alpha + \\beta} = \\frac{16 + 1}{100 + 1 + 1} = 0.1667$ \n",
    "\n",
    "$E(N = 1 | (A = 1, H = 0, C = 1)) = \\frac{\\sum_{i = 1}^N x_i + \\alpha}{N + \\alpha + \\beta} = \\frac{4 + 1}{54 + 1 + 1} = 0.0893$ \n",
    "\n",
    "$E(N = 1 | (A = 0, H = 1, C = 1)) = \\frac{\\sum_{i = 1}^N x_i + \\alpha}{N + \\alpha + \\beta} = \\frac{14 + 1}{121 + 1 + 1} = 0.1220$ \n",
    "\n",
    "$E(N = 1 | (A = 1, H = 1, C = 1)) = \\frac{\\sum_{i = 1}^N x_i + \\alpha}{N + \\alpha + \\beta} = \\frac{26 + 1}{71 + 1 + 1} = 0.3699$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With beta priors that show strong weighting for low likelihood of a book purchase, prior $Beta(1,3)$: \n",
    "\n",
    "$E(\\theta_{A} | x) =  \\frac{\\sum_{i = 1}^N x_i + \\alpha}{N + \\alpha + \\beta} = \\frac{301 + 1}{1000 + 1 + 3} = 0.3008$\n",
    "\n",
    "$E(\\theta_{H} | x) =  \\frac{\\sum_{i = 1}^N x_i + \\alpha}{N + \\alpha + \\beta} = \\frac{543 + 1}{1000 + 1 + 3} = 0.5418$\n",
    "\n",
    "$E(\\theta_{C} | x) =  \\frac{\\sum_{i = 1}^N x_i + \\alpha}{N + \\alpha + \\beta} = \\frac{380 + 1}{1000 + 1 + 3} = 0.3795$\n",
    "\n",
    "$E(N = 1 | (A = 0, H = 0, C = 0)) = \\frac{\\sum_{i = 1}^N x_i + \\alpha}{N + \\alpha + \\beta} = \\frac{4 + 1}{193 + 1 + 3} = 0.0254$ \n",
    "\n",
    "$E(N = 1 | (A = 1, H = 0, C = 0)) = \\frac{\\sum_{i = 1}^N x_i + \\alpha}{N + \\alpha + \\beta} = \\frac{12 + 1}{76 + 1 + 3} = 0.1625$ \n",
    "\n",
    "$E(N = 1 | (A = 0, H = 1, C = 0)) = \\frac{\\sum_{i = 1}^N x_i + \\alpha}{N + \\alpha + \\beta} = \\frac{10 + 1}{251 + 1 + 3} = 0.0431$ \n",
    "\n",
    "$E(N = 1 | (A = 0, H = 0, C = 1)) = \\frac{\\sum_{i = 1}^N x_i + \\alpha}{N + \\alpha + \\beta} = \\frac{3 + 1}{134 + 1 + 3} = 0.0290$ \n",
    "\n",
    "$E(N = 1 | (A = 1, H = 1, C = 0)) = \\frac{\\sum_{i = 1}^N x_i + \\alpha}{N + \\alpha + \\beta} = \\frac{16 + 1}{100 + 1 + 3} = 0.1635$ \n",
    "\n",
    "$E(N = 1 | (A = 1, H = 0, C = 1)) = \\frac{\\sum_{i = 1}^N x_i + \\alpha}{N + \\alpha + \\beta} = \\frac{4 + 1}{54 + 1 + 3} = 0.0862$ \n",
    "\n",
    "$E(N = 1 | (A = 0, H = 1, C = 1)) = \\frac{\\sum_{i = 1}^N x_i + \\alpha}{N + \\alpha + \\beta} = \\frac{14 + 1}{121 + 1 + 3} = 0.12$ \n",
    "\n",
    "$E(N = 1 | (A = 1, H = 1, C = 1)) = \\frac{\\sum_{i = 1}^N x_i + \\alpha}{N + \\alpha + \\beta} = \\frac{26 + 1}{71 + 1 + 3} = 0.36$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When comparing the posterior mean with prior $Beta(1,1)$ and the posterior mean with prior $Beta(1,3)$, we can see that the posterior mean only change by a small amount."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "(20) The data set CHDdata.csv contains cases of coronary heart disease\n",
    "(CHD) and variables associated with the patient's condition: systolic\n",
    "blood pressure, yearly tobacco use (in kg), low density lipoprotein (ldl),\n",
    "adiposity, family history (0 or 1), type A personality score (typea), obesity\n",
    "(body mass index), alcohol use, age, and the diagnosis of CHD (0 or 1).\n",
    "**Perform a Bayesian analysis of these data that finds the posterior marginal\n",
    "probability distributions for the means for the data of patients with and\n",
    "without CHD.** \n",
    "1. You should first standard scale (subtract the mean and divide by the standard deviation) all the numeric variables (remove family history and do not scale CHD). \n",
    "2. Then separate the data into two sets, one for patients with CHD and one for patients without CHD.\n",
    "    - Your priors for both groups should assume means of 0 for all variables and a correlation of 0 between all pairs of variables. You should assume all variances for the variables are 1. \n",
    "3. Use a prior alpha equal to one plus the number of predictor variables. Compute and compare the Bayesian estimates for the posterior means for each group.\n",
    "4. For 5 extra credit points, compute the probability of observing a point at least as extreme as the posterior mean of patients without CHD under the posterior distribution for the patients with CHD. Then compute the probability of observing a point at least as extreme as the posterior mean of patients with CHD under the posterior distribution for the patients without CHD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sbp</th>\n",
       "      <th>tobacco</th>\n",
       "      <th>ldl</th>\n",
       "      <th>adiposity</th>\n",
       "      <th>typea</th>\n",
       "      <th>obesity</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>age</th>\n",
       "      <th>chd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.057417</td>\n",
       "      <td>1.821099</td>\n",
       "      <td>0.477894</td>\n",
       "      <td>-0.295183</td>\n",
       "      <td>-0.418017</td>\n",
       "      <td>-0.176594</td>\n",
       "      <td>3.274189</td>\n",
       "      <td>0.628654</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.276789</td>\n",
       "      <td>-0.789382</td>\n",
       "      <td>-0.159507</td>\n",
       "      <td>0.411694</td>\n",
       "      <td>0.193134</td>\n",
       "      <td>0.670646</td>\n",
       "      <td>-0.612081</td>\n",
       "      <td>1.381617</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.991731</td>\n",
       "      <td>-0.774141</td>\n",
       "      <td>-0.608585</td>\n",
       "      <td>0.883374</td>\n",
       "      <td>-0.112441</td>\n",
       "      <td>0.734723</td>\n",
       "      <td>-0.540597</td>\n",
       "      <td>0.217947</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.545310</td>\n",
       "      <td>0.841352</td>\n",
       "      <td>0.806252</td>\n",
       "      <td>1.622382</td>\n",
       "      <td>-0.214300</td>\n",
       "      <td>1.411091</td>\n",
       "      <td>0.294742</td>\n",
       "      <td>1.039361</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.211103</td>\n",
       "      <td>2.169453</td>\n",
       "      <td>-0.598928</td>\n",
       "      <td>0.305020</td>\n",
       "      <td>0.702427</td>\n",
       "      <td>-0.012842</td>\n",
       "      <td>1.645991</td>\n",
       "      <td>0.423301</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sbp   tobacco       ldl  adiposity     typea   obesity   alcohol  \\\n",
       "0  1.057417  1.821099  0.477894  -0.295183 -0.418017 -0.176594  3.274189   \n",
       "1  0.276789 -0.789382 -0.159507   0.411694  0.193134  0.670646 -0.612081   \n",
       "2 -0.991731 -0.774141 -0.608585   0.883374 -0.112441  0.734723 -0.540597   \n",
       "3  1.545310  0.841352  0.806252   1.622382 -0.214300  1.411091  0.294742   \n",
       "4 -0.211103  2.169453 -0.598928   0.305020  0.702427 -0.012842  1.645991   \n",
       "\n",
       "        age  chd  \n",
       "0  0.628654    1  \n",
       "1  1.381617    1  \n",
       "2  0.217947    0  \n",
       "3  1.039361    1  \n",
       "4  0.423301    1  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "# Import Data\n",
    "data = pd.read_csv(\"CHDdata.csv\")\n",
    "# Remove famhist and chd\n",
    "data_scale = data.drop('famhist', axis = 1).drop('chd', axis = 1)\n",
    "\n",
    "# Scale the columns\n",
    "data_scale = data_scale - data_scale.mean()\n",
    "data_scale = data_scale / data_scale.std()\n",
    "\n",
    "# Add the chd column back\n",
    "data_scale = pd.concat([data_scale.reset_index(drop = True), \n",
    "                        data['chd']], axis = 1)\n",
    "data_scale.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into 2 datasets\n",
    "data_with_chd = data_scale[data_scale['chd'] == 1].drop('chd', axis = 1)\n",
    "data_without_chd =  data_scale[data_scale['chd'] == 0].drop('chd', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know the posterior mean is \n",
    "$$\\mu^* = \\frac{v\\mu_0 + N \\bar{x}}{v + N }$$\n",
    "and in this case, $v = 0$, $\\mu_0$ is a vector of zeros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sbp          0.262342\n",
       "tobacco      0.408770\n",
       "ldl          0.358765\n",
       "adiposity    0.346584\n",
       "typea        0.140689\n",
       "obesity      0.136515\n",
       "alcohol      0.085283\n",
       "age          0.508680\n",
       "dtype: float64"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For people with CHD, the posterior mean is \n",
    "count_chd = data_with_chd.count()[1]\n",
    "mu_chd = count_chd * data_with_chd.mean() / (1 + count_chd)\n",
    "mu_chd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sbp          0.263111\n",
       "tobacco      0.409967\n",
       "ldl          0.359816\n",
       "adiposity    0.347599\n",
       "typea        0.141101\n",
       "obesity      0.136915\n",
       "alcohol      0.085532\n",
       "age          0.510170\n",
       "dtype: float64"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For people without CHD, the posterior mean is \n",
    "count_nchd = data_without_chd.count()[1]\n",
    "mu_nchd = count_nchd*data_with_chd.mean() / (1 + count_nchd)\n",
    "mu_nchd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precision matrix $r^*$ is defined as\n",
    "$$r^* = r + S + \\frac{vN}{v+ N} (\\mu_0 - \\bar{x})(\\mu_0 - \\bar{x})^T  $$\n",
    "where $S = \\sum_{i = 1}^N (x_i - \\bar{x}) (x_i - \\bar{x})^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the precision matrix for CHD patients\n",
    "S = 0\n",
    "for i in range(0, count_chd):\n",
    "    x1 = data_with_chd.iloc[i]  - data_with_chd.mean() \n",
    "    x2 = np.transpose(data_with_chd.iloc[i]  - data_with_chd.mean())\n",
    "    S += np.matmul(x1, x2)\n",
    "r_chd = 1 + S + (count_chd) / (1 + count_chd) * np.dot((0 - np.array(data_with_chd.mean()).reshape((8,1))), \n",
    "                                                       np.transpose(0 - np.array(data_with_chd.mean()).reshape((8,1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 8)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the precision matrix for non-CHD patients\n",
    "S = 0\n",
    "for i in range(0, count_chd):\n",
    "    x1 = data_without_chd.iloc[i]  - data_without_chd.mean() \n",
    "    x2 = np.transpose(data_without_chd.iloc[i]  - data_without_chd.mean())\n",
    "    S += np.matmul(x1, x2)\n",
    "r_nchd = 1 + S + (count_chd) / (1 + count_chd) * np.dot((0 - np.array(data_without_chd.mean()).reshape((8,1))), \n",
    "                                                       np.transpose(0 - np.array(data_without_chd.mean()).reshape((8,1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $S_{pooled}$ be defined as: \n",
    "$$S_{pooled} = \\frac{(N_{CHD} - 1) [r_{CHD}]^{-1} + (N_{Normal} - 1) [r_{Normal}]^{-1}}{(N_{CHD} - 1) + (N_{Normal} - 1)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_pooled = ((count_chd - 1) *  np.linalg.inv(r_chd)  + \\\n",
    "            (count_nchd - 1) *  np.linalg.inv(r_nchd)) \\\n",
    "            /(count_chd - 1 + count_nchd - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$F = |\\mu_{CHD} - \\mu_{Normal}|^T * S_{pooled} * |\\mu_{CHD} - \\mu_{Normal}|$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.30384515]])"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu_diff = abs(np.array(data_with_chd.mean()).reshape((8,1)) - np.array(data_without_chd.mean()).reshape((8,1)))\n",
    "F = np.dot(np.dot(np.transpose(mu_diff), S_pooled),mu_diff)\n",
    "F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing an F test\n",
    "\n",
    "varaince and covariance, use the R, take the inverse this, \n",
    "\n",
    "See Piazza, matrix multiplication in python\n",
    "\n",
    "where the equations are: slides 8-9 multi conjugate prior\n",
    "posterior mean and posterior precision, just calculating those\n",
    "\n",
    "looking for is there a difference between people on this variables\n",
    "\n",
    "is there a significant difference between these two gaussian distribution \n",
    "\n",
    "use F teston the mean, pooled F test, by inverting the precise to calculate the variance-covariance matrix. \n",
    "\n",
    "$N_{CHD} = $ number of CHD patients, \n",
    "$N_{Normal} = $ number of normal patients\n",
    "S is the varaince covariance matrix\n",
    "r is the precision\n",
    "$$S_{pooled} = \\frac{(N_{CHD} - 1) [r_{CHD}]^{-1} + (N_{Normal} - 1) [r_{Normal}]^{-1}}{(N_{CHD} - 1) + (N_{Normal} - 1)}$$\n",
    "\n",
    "\n",
    "$F^* = |\\mu_{CHD} - \\mu_{Normal}|^T * S_{pooled} * |\\mu_{CHD} - \\mu_{Normal}|$ to get a scaler and compare this with the F value. \n",
    "\n",
    "95%, F(0.95, df = 9 number of parameters, number of observations 462 - 2 - 9)\n",
    "\n",
    "Compare the test statistic and the critical F"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
