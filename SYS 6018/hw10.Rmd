---
title: 'Homework #10'
---

#### Congxin (David) Xu
#### Computing ID: cx2rx

**SYS 6018 | Fall 2020 | University of Virginia **

*******************************************

<!--- Below are global settings for knitr. You can override any of them by adding the changes to individual chunks --->

```{r global_options, include=FALSE}
knitr::opts_chunk$set(error=TRUE,        # Keep compiling upon error
                      collapse=FALSE,    # collapse by default
                      echo=TRUE,         # echo code by default
                      comment = "#>",    # change comment character
                      fig.width = 5,     # set figure width
                      fig.align = "center",# set figure position
                      out.width = "49%", # set width of displayed images
                      warning=TRUE,      # show R warnings
                      message=TRUE)      # show R messages
options(dplyr.summarise.inform = FALSE)  # ignore message about group structure
```

<!--- Solution Region --->
```{css solution-region, echo=FALSE}
.solution {
  background-color: #232D4B10;
  border-style: solid;
  border-color: #232D4B;
  padding: .5em;
  margin: 20px
}
```


<!--- Load Required R packages here --->
```{r packages, include=FALSE}
#- Better table printing
library(kableExtra) # https://haozhu233.github.io/kableExtra/awesome_table_in_html.html
format_table <- function(x, nmax=10, ...) {
  kable(x, ...) %>% 
    kable_styling(full_width = FALSE, font_size=11, position = "left") %>% 
    {if(nrow(x) > nmax) scroll_box(., width = "100%", height = "300px") else .}
}
#- useful functions
digits <- function(x, k=2) format(round(x, k), nsmall=k)
#- data directory
data.dir = 'https://mdporter.github.io/SYS6018/data/'
#- required functions here
library(igraph)
library(tidygraph)
library(tidyverse)
```

### Problem 10.1 The Marvel Universe

[Alberich, Miro-Julia, \& Rossell&oacute; (2002)](https://arxiv.org/pdf/cond-mat/0202174.pdf) 
examined the social network structure of the Marvel Comics Universe and found some similarities to real-world collaboration networks. 
The folks at <http://syntagmatic.github.io/exposedata/marvel/> have made the network data available (along with some nice visualizations). I have extracted the Hero Social Network Data which can be accessed using [this link](https://mdporter.github.io/SYS6018/data/marvel_hero-network.csv). Note that the data contain one edge for each time two heroes appeared in the same comic.  


a. Load the data and make a *weighted* and *undirected* graph, where the `weight` corresponds to the number of times the heroes appeared in the same comic. Ensure your graph has an edge attribute named `weight`. The weight between *LITTLE, ABNER* and *BLACK PANTHER/T'CHAL LITTLE* should be 7. 
    - No need to make a plot, just show your code to make the graph object.

<div class="solution"> 
```{r 10.1 a, message=FALSE}
# Read in the data
marvel <- read_csv("https://mdporter.github.io/SYS6018/data/marvel_hero-network.csv",
                   col_types = 
                     cols(
                       from = col_character(),
                       to = col_character()))

# Create the weight
marvel <- marvel %>% 
  group_by(from, to) %>% 
  summarize(weight = n())

# Create the unique nodes 
nodes <- data.frame(node = c(marvel$from, marvel$to),
                    stringsAsFactors = F) %>% distinct()

# Generate the graph
g <- tbl_graph(nodes = nodes, edges = marvel, directed = FALSE, node_key = 'node')
```
</div>



b. Run the *fast-greedy* community detection algorithm (`igraph::cluster_fast_greedy()`).
    - Use the edge weights in the community detection algorithm.
    - How many communities did it find? 
    - Use a plot to show community size of each group (i.e., group number on the x-axis and group size on y-axis).

<div class="solution"> 
```{r 10.1 b}
# Run the fast-greedy community detection algorithm
fg <- cluster_fast_greedy(g)

# Print the total number of communities
print(max(fg$membership))

# Plot the community size of each group
ggplot(data = data.frame(x = fg$membership)) + geom_bar(aes(x = x)) + 
  ggtitle('Community Size by Membership') +
  xlab('Community') +
  ylab('Frequency')
```
The fast-greedy community detection algorithm find 46 communities.
</div>



c. Calculate the following centrality scores for the hero network: *eigenvector, betweeness, and degree*. 
    - `igraph` has two versions of centrality calculations (I know, a bit confusing).
    - The ones starting with `centr_` do not consider edge weights.
    - The others (e.g., `betweenness()`, `eigen_centrality()`) will allow weights.
    - For this exercise, ignore the weights and use the `centr_` versions. 
    - By default, these will return a normalized version (which divides the score by the theoretical maximum value). 
    - Show the top 10 heroes arranged by *eigenvector centrality*. 
    - Which hero has the largest eigenvector centrality? How does this make the hero *important*? 

<div class="solution"> 
```{r 10.1 c}
nodes <- nodes %>% 
  mutate(eigenvector = centr_eigen(g)$vector, 
         betweenness = centr_betw(g)$res,
         degree = centr_degree(g)$res) %>% 
  arrange(desc(eigenvector))

head(nodes, 10)
```

**Captain America** is has the largest eigenvector centrality. This means that Captain America node is connected with all other important character nodes within the network.

</div>

d. For each of the three largest communities find the hero with the largest *betweenness centrality*. Explain how these heroes are *important*. 


<div class="solution"> 
```{r 10.1 d, message=FALSE}
nodes <- nodes %>% 
  mutate(membership = fg$membership)

nodes %>% 
  group_by(membership) %>% 
  summarize(count = n()) %>% 
  arrange(desc(count)) %>% 
  top_n(3) %>% 
  inner_join(nodes, by = 'membership') %>% 
  group_by(membership) %>% 
  filter(betweenness == max(betweenness)) %>% 
  select(membership, node, betweenness)
```
We can see the hers with the largest betweenness centrality from the table above. There heroes are important because lots of information flows through the nodes that represents these heroes. 
</div>


### Problem 10.2: Alpha Centrality


[Bonacich and Lloyd (2001)](https://github.com/mdporter/SYS6018/raw/master/other/alpha-centrality_Bonacich.pdf) introduced *alpha centrality* as an alternative to eigenvector centrality. Their main idea is that the importance of a node is based on the network structure **plus** some known external sources of importance. The alpha centrality vector $x$ is defined:
\[ 
x = \alpha  A^T x + s
\]
where $s$ is the vector of exogenous importance and $0 \leq \alpha \leq 1/\lambda_1$ (where $\lambda_1$ is the maximum eigenvalue of $A$) reflects the relative importance of the endogenous factors of importance.


a. PageRank can be considered a special case of alpha centrality. What does PageRank use for $s$, $\alpha$, and $A$? Use the notation from the class notes,  e.g., $\alpha=d$. 

<div class="solution"> 

PageRank uses $s = 0$ , $\alpha = \frac{1}{d^{out}_i}$ and $A = A_{ij}$ where $d^{out}_i$ is the degree of the node i and $A_{ij}$ is the adjacency matrix of the network.

</div>

---

<div style="background-color:lightgrey; display: block; border-color: black; padding:1em">

The next few problems will explore how alpha centrality can be used for identifying the bad actors in the money laundering data. The money laundering data was used in class and can be accessed here:

- nodes: <https://mdporter.github.io/SYS6018/data/transfers_nodes.csv>

- edges: <https://mdporter.github.io/SYS6018/data/transfers.csv>

</div>


b. Make a *directed* graph from these data. 
    - Show code, no need to make a plot
    - Note: the `time` column may cause a message when you create the igraph object. We don't use time for this problem, so it can be safely ignored.  

<div class="solution"> 
```{r 10.2 b, message=FALSE}
nodes <- read_csv("https://mdporter.github.io/SYS6018/data/transfers_nodes.csv")
nodes <- nodes %>% 
  mutate(fraud = ifelse(is.na(fraud), "NA", fraud)) %>% 
  mutate(prior = ifelse(fraud == T, 1, 0)) %>% 
  mutate(prior = ifelse(fraud == 'NA', 0.01, prior))

edges <- read_csv("https://mdporter.github.io/SYS6018/data/transfers.csv")

g <- tbl_graph(nodes = nodes, edges = edges, directed = T, node_key = 'node')
```

</div>

c. Using the *directed graph*, set $s=1$ for the known fraudsters, $s=0$ for the legitimate, and $s=0.01$ for the unknown nodes and calculate the alpha centrality. You can think of $s$ as proportional to the prior probability that a node is a fraudster. 
    - Use $\alpha = 0.8$. 
    - Use a Cleveland dot plot (or bar plot) to visually display the alpha centrality scores for all node. Use color (or shape) to distinguish between the fraud, non-fraud, and unknown nodes. 
    - Comment on what this tells you about the two unknown nodes

<div class="solution"> 

```{r 10.2 c}
# Create the alpha centrality values
nodes <- nodes %>% 
  mutate(alpha_centr = alpha.centrality(g, alpha = 0.8, exo = nodes$prior))

# Create a bar chart for the each id
ggplot(nodes, aes(x = id, y = alpha_centr)) + geom_col(aes(fill = fraud))
```
Based on the alpha centrality values, the two unknown nodes are likely to be the non-fraudsters, because its alpha centrality are very similar to other non-fraudsters.

</div>


### Problem 10.3: Hubs and Authorities (HITS) 

The HITS algorithm is described in [MMDS 5.5](http://infolab.stanford.edu/~ullman/mmds/ch5.pdf)


a. The HITS scores are designed to work with *directed* networks. What is the result of running HITS on an *undirected* network? Show that the scores reduce to a familiar centrality score. 

<div class="solution"> 

The result of running HITS on an undirected network is eigenvector centrality.

</div>


b. Write a function to calculate the Hubs and Authority scores. See MMDS 5.5.2 for details. 

<div class="solution"> 
Add Solution Here
</div>


c. Use your function to calculate the hubs and authority scores for the Political Blog data [Adamic and Glance (2005). "The political blogosphere and the 2004 U.S. election: divided they blog", In Proceedings of the *3rd International Workshop on Link discovery (LinkKDD '05)*. ACM, New York, NY, USA](https://mdporter.github.io/SYS6018/other/(Adamic)%20Political%20Blogs.pdf).

- nodes: <https://mdporter.github.io/SYS6018/data/polblogs_nodes.csv>

- edges: <https://mdporter.github.io/SYS6018/data/polblogs.csv>

The `nodes` data has a column named `leaning` which indicates the political leaning (liberal or conservative) of the blog and a column named `label` which gives the blog name. 

Run HITS on the full data, and then report the top 5 hubs and top 5 authority scores (with blog name) for both liberal and conservative blogs.

- In the case of a failure in part b, use the igraph functions, e.g. `hub.score()` and `authority.score()`.   
- Note: the network is *directed*
 

<div class="solution"> 
Add Solution Here
</div>











 


