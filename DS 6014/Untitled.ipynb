{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LDA_wikipedia:\n",
    "    \"\"\"Creates a class for Latent Dirichlet Allocation using summaries from Wikipedia\n",
    "    Input:\n",
    "        title_list = list of titles for Wikipedia pages\n",
    "        N_topics = number of topics for LDA to produce\n",
    "        N_words = the number of words to show in a topic\n",
    "        new_title = title for a new page not in the training s\n",
    "    Methods:\n",
    "        Topics = Outputs the list of topics in the selected Wikipedia pages as a dataframe\n",
    "        Predict_Topics\n",
    "            Input: New titles for Wikipedia pages\n",
    "            Output: A dataframe with the probabilities for topics for each new page\"\"\"\n",
    "    \n",
    "    def __init__(self, title_list, N_topics=3, N_words = 10):\n",
    "        # initialize variables\n",
    "        self.title_list = title_list\n",
    "        self.N_topics = N_topics\n",
    "        self.N_words = N_words\n",
    "        # start with an empty corpus\n",
    "        self.corpus = list()\n",
    "    \n",
    "        # Get the summary pages for the given titles\n",
    "        # then preprocess\n",
    "        for title in self.title_list:\n",
    "            page = wikipedia.page(title)\n",
    "            self.corpus.append(preprocessor(page.summary))\n",
    "        \n",
    "        # Get the matrix of word counts for the pages\n",
    "        # this will be the input the the LDA\n",
    "        self.countVectorizer = CountVectorizer(stop_words='english')\n",
    "        self.termFrequency = self.countVectorizer.fit_transform(self.corpus)\n",
    "        self.Words = self.countVectorizer.get_feature_names()\n",
    "        \n",
    "    def Topics(self):\n",
    "        # Obtain the estimates for the LDA model \n",
    "        self.lda = LatentDirichletAllocation(n_components=self.N_topics)\n",
    "        self.lda.fit(self.termFrequency)\n",
    "        \n",
    "        # Obtain the list of the top N_words in the topics\n",
    "        topics = list()\n",
    "        for topic in self.lda.components_:\n",
    "            topics.append([self.Words[i] for i in topic.argsort()[:-self.N_words - 1:-1]])\n",
    "            \n",
    "        # Create a list of column names, Words, for the dataframe output\n",
    "        cols = list()\n",
    "        for i in range(self.N_words):\n",
    "            cols.append(\"Word \"+(str(i)))\n",
    "        \n",
    "        # Create a dataframe with the topic no. and the words in each topic \n",
    "        # output this dataframe\n",
    "        Topics_df = pd.DataFrame(topics, columns = cols)\n",
    "        Topics_df.index.name = \"Topics\"\n",
    "        return Topics_df  \n",
    "    \n",
    "    def Predict_Topics(self, new_title_list):\n",
    "        # Get the new titles for the new pages\n",
    "        # and the number of new pages \n",
    "        self.new_title_list = new_title_list\n",
    "        N_new_docs = len(new_title_list)\n",
    "        \n",
    "        # For each of the new titles get the summary page in Wikipedia\n",
    "        # then obtain the estimate probabilities for each of the topics\n",
    "        # discovered in the training set for each of the new pages\n",
    "        new_doc_topics = list()\n",
    "        for title in self.new_title_list:\n",
    "            new_page = wikipedia.page(title)\n",
    "            new_doc = preprocessor(new_page.summary)\n",
    "            new_doc_topics.append(self.lda.transform(self.countVectorizer.transform([new_doc])))\n",
    "            \n",
    "        # Recast the list of topic probabilities as an array of size number of no. pages X no. of topics\n",
    "        new_doc_topics = np.array(new_doc_topics).reshape(N_new_docs, self.N_topics)\n",
    "        # Create labels for the columns in the output dataframe\n",
    "        cols = list()\n",
    "        for i in range(self.N_topics):\n",
    "            cols.append(\"Topic \"+(str(i)))\n",
    "            \n",
    "        # Create the dataframe whose rows contain the topic probabilities for specific Wikipedia pages\n",
    "        New_Page_df = pd.DataFrame(new_doc_topics, columns = cols )\n",
    "        New_Page_df.insert(0, 'Page Name', self.new_title_list)\n",
    "        return New_Page_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
