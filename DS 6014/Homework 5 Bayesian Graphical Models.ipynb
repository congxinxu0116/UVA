{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5: Bayesian Graphical Models\n",
    "\n",
    "- Name: Congxin (David) Xu\n",
    "- Computing ID: cx2rx\n",
    "\n",
    "### Honor Pledge: \n",
    "I have neither given nor received aid on this assignment.\n",
    "\n",
    "### Problem 1\n",
    "(30) This problem explores the use of variational approximation in Latent\n",
    "Dirichlet Allocation (LDA). We will use the implementation in sklearn of\n",
    "the variational approximation algorithm in [1].\n",
    "\n",
    "#### Part a\n",
    "\n",
    "Use the notation in the diagram in Figure 1 [2] to write the target\n",
    "posterior distribution of the latent variables and parameters for the\n",
    "general LDA method. Why do we use variational approximation\n",
    "rather than conjugate priors or sampling to obtain this posterior\n",
    "distribution?\n",
    "\n",
    "**Answer**\n",
    "\n",
    "The posterior distribution of the latent variables is \n",
    "$$ p(\\theta, z | w, \\alpha, \\beta) = \\frac{p(\\theta, z, w| \\alpha, \\beta)}{p(w | \\alpha, \\beta)}  $$\n",
    "\n",
    "The reason to use variational approximation rather than conjugate priors or sampling to obtain this posterior distribution is becuase for many models, it is intractable to compute the posterior distribution. Sampling methods are limited to smaller data problems but variational approximation can handle large problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part b\n",
    "\n",
    "Accident reports provide a good use-case for LDA since the narrative\n",
    "information in these reports is frequently overlooked in safety analysis. LDA allows us to capture elements (topics) in this narrative\n",
    "data and use them to better understand unsafe conditions. For this\n",
    "use-case, modify the LDA class for Wikipedia in the `LDA Examples Wikipedia and Trains` jupyter notebook to perform LDA on\n",
    "the accident narratives. About 10 years of these narratives are in\n",
    "the json file, `TrainNarratives.txt`. Use this class to obtain 10\n",
    "topics from the accident narratives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import wikipedia\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "\n",
    "# Set stop words\n",
    "stopWords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDA_trains:\n",
    "    \"\"\"Creates a class for Latent Dirichlet Allocation using summaries from Wikipedia\n",
    "    Input:\n",
    "        reports = list of narratives from accident reports\n",
    "        N_topics = number of topics for LDA to produce\n",
    "        N_words = the number of words to show in a topic\n",
    "        new_report = narrative for a new accident report not in the training set\n",
    "    Methods:\n",
    "        Topics = output the list of topics in the selected narratives\n",
    "        Predict_Topics = Show the predicted probabilities for topics for a new accident narrative\n",
    "            Input: new narrative\n",
    "            \"\"\"\n",
    "    def __init__(self, reports, N_topics=3, N_words = 10):\n",
    "        # the narrative reports\n",
    "        self.reports = reports\n",
    "        # initialize variables\n",
    "        self.N_topics = N_topics\n",
    "        self.N_words = N_words\n",
    "        \n",
    "        # Get the word counts in the reports\n",
    "        self.countVectorizer = CountVectorizer(stop_words='english')\n",
    "        self.termFrequency = self.countVectorizer.fit_transform(self.reports)\n",
    "        self.Words = self.countVectorizer.get_feature_names()\n",
    "        \n",
    "    def Topics(self):\n",
    "                \n",
    "        # Obtain the estimates for the LDA model \n",
    "        self.lda = LatentDirichletAllocation(n_components=self.N_topics)\n",
    "        self.lda.fit(self.termFrequency)\n",
    "        \n",
    "        # Obtain the list of the top N_words in the topics\n",
    "        topics = list()\n",
    "        for topic in self.lda.components_:\n",
    "            topics.append([self.Words[i] for i in topic.argsort()[:-self.N_words - 1:-1]])\n",
    "            \n",
    "        # For each of the topics in the model add the top N_words the list of topics\n",
    "        ### Your code here\n",
    "        # Create column names for the output matrix\n",
    "        cols = list()\n",
    "        for i in range(self.N_words):\n",
    "            cols.append(\"Word \" + (str(i)))\n",
    "            \n",
    "        # Create a dataframe with the topic no. and the words in each topic \n",
    "        # output this dataframe \n",
    "        Topics_df = pd.DataFrame(topics, columns = cols)\n",
    "        Topics_df.index.name = \"Topics\"\n",
    "        return Topics_df\n",
    "    \n",
    "    def Predict_Topics(self, new_reports):\n",
    "        self.new_reports = new_reports\n",
    "        \n",
    "        # Get the list of new accident report narratives\n",
    "        # and the number of new narratives\n",
    "        N_new_reports = len(self.new_reports)\n",
    "        \n",
    "        \n",
    "        # For each of the new narratives \n",
    "        # obtain the estimated probabilities for each of the topics\n",
    "        # in each of the new narratives as estimated by the LDA results\n",
    "        # on the training set \n",
    "        new_report_topics = list()\n",
    "        ### Your code here        \n",
    "        for i in self.new_reports:\n",
    "            new_report_topics.append(self.lda.\\\n",
    "                                     transform(self.countVectorizer.\\\n",
    "                                               transform([i])))\n",
    "        \n",
    "        # Recast the list of probabilities for topics as an array \n",
    "        # of size no. of new reports X no. of topics\n",
    "        new_report_topics = np.array(new_report_topics).\\\n",
    "            reshape(N_new_reports, self.N_topics)\n",
    "        \n",
    "        # Create column names for the output dataframe\n",
    "        cols = list()\n",
    "        ### Your code here        \n",
    "        for i in range(self.N_topics):\n",
    "            cols.append(\"Topic \"+(str(i)))\n",
    "            \n",
    "        # Create the dataframe whose rows contain topic probabilities for \n",
    "        # specificed narratives/reports\n",
    "        ### Your code here\n",
    "        New_Reports_df = pd.DataFrame(new_report_topics, columns = cols)        \n",
    "        New_Reports_df.insert(0, 'Reports', self.new_reports)\n",
    "        \n",
    "        return New_Reports_df\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['UNITS 231-281(BACK TO BACK)  WERE COMING INTO UP DEISEL SHOP  WHEN THE LEFT WHEEL OF 281 RODE OVER RECENTLY REPAIRED SWITCH PLATE AND DERAILED. THE CAUSE WAS DETERMINED TO BE THE TRACK TELEMETRY IN THAT IT WAS TOO SHARP OF A CURVE.',\n",
       " 'ENGINE 286 CAUGHT FIRE AT THE SPRINGFIELD, MA STATION DUE TO BEARINGS IN MAIN GENERATOR LET GO.',\n",
       " 'TRAIN NO.#4 WITH ENGS 83/11/90/44 AND 11 CARS DERAILED 2 DEADHEAD CARS, C/44834 AND C/9639, WHILE MAKING A SHOVING MOVE ONTO TRACK 28.  THE DERAILMENT WAS DUE TO HIGH BUFF FORCES CAUSED JACKKNIFING OFDEADHEADING AMFLEET CAR 44834 LOCATED DIRECTLY BEHIND ENGINES DUE TO EXCESSIVE AMPERAGE GENERATED BY FOUR P42 LOCOMOTIVES SHOVING TRAIN AGAINST AN APPROXIMATELY 15-POUND BRAKE REDUCTION.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train accident narratives are in a json file\n",
    "# Read the JSON file with the narratives and convert to a list for the LDA analysis\n",
    "\n",
    "with open('TrainNarratives.txt') as json_file:  \n",
    "    Narrative_dict = json.load(json_file)\n",
    "    \n",
    "train_reports = list(Narrative_dict.values())\n",
    "    \n",
    "train_reports[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word 0</th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>Word 3</th>\n",
       "      <th>Word 4</th>\n",
       "      <th>Word 5</th>\n",
       "      <th>Word 6</th>\n",
       "      <th>Word 7</th>\n",
       "      <th>Word 8</th>\n",
       "      <th>Word 9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topics</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fuel</td>\n",
       "      <td>gallons</td>\n",
       "      <td>released</td>\n",
       "      <td>cars</td>\n",
       "      <td>diesel</td>\n",
       "      <td>spilled</td>\n",
       "      <td>gal</td>\n",
       "      <td>tank</td>\n",
       "      <td>lead</td>\n",
       "      <td>bnsf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cars</td>\n",
       "      <td>track</td>\n",
       "      <td>rail</td>\n",
       "      <td>derailed</td>\n",
       "      <td>car</td>\n",
       "      <td>switch</td>\n",
       "      <td>crew</td>\n",
       "      <td>derail</td>\n",
       "      <td>causing</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>car</td>\n",
       "      <td>derailed</td>\n",
       "      <td>track</td>\n",
       "      <td>cars</td>\n",
       "      <td>causing</td>\n",
       "      <td>end</td>\n",
       "      <td>switch</td>\n",
       "      <td>lead</td>\n",
       "      <td>shoving</td>\n",
       "      <td>derail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ns</td>\n",
       "      <td>derailed</td>\n",
       "      <td>loads</td>\n",
       "      <td>tons</td>\n",
       "      <td>empties</td>\n",
       "      <td>pulling</td>\n",
       "      <td>units</td>\n",
       "      <td>wheels</td>\n",
       "      <td>cars</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>car</td>\n",
       "      <td>cars</td>\n",
       "      <td>track</td>\n",
       "      <td>cut</td>\n",
       "      <td>bowl</td>\n",
       "      <td>humping</td>\n",
       "      <td>hump</td>\n",
       "      <td>rolled</td>\n",
       "      <td>humped</td>\n",
       "      <td>damage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>train</td>\n",
       "      <td>emergency</td>\n",
       "      <td>derailed</td>\n",
       "      <td>crew</td>\n",
       "      <td>went</td>\n",
       "      <td>mp</td>\n",
       "      <td>cars</td>\n",
       "      <td>conductor</td>\n",
       "      <td>traveling</td>\n",
       "      <td>stopped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>train</td>\n",
       "      <td>engineer</td>\n",
       "      <td>car</td>\n",
       "      <td>cars</td>\n",
       "      <td>conductor</td>\n",
       "      <td>stop</td>\n",
       "      <td>brake</td>\n",
       "      <td>derailment</td>\n",
       "      <td>went</td>\n",
       "      <td>slack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>derailed</td>\n",
       "      <td>cars</td>\n",
       "      <td>track</td>\n",
       "      <td>rail</td>\n",
       "      <td>pulling</td>\n",
       "      <td>train</td>\n",
       "      <td>broken</td>\n",
       "      <td>hazardous</td>\n",
       "      <td>main</td>\n",
       "      <td>materials</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>track</td>\n",
       "      <td>cars</td>\n",
       "      <td>switch</td>\n",
       "      <td>crew</td>\n",
       "      <td>yard</td>\n",
       "      <td>lead</td>\n",
       "      <td>end</td>\n",
       "      <td>derailed</td>\n",
       "      <td>cut</td>\n",
       "      <td>lined</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>damage</td>\n",
       "      <td>train</td>\n",
       "      <td>pantograph</td>\n",
       "      <td>track</td>\n",
       "      <td>equipment</td>\n",
       "      <td>struck</td>\n",
       "      <td>wire</td>\n",
       "      <td>causing</td>\n",
       "      <td>operating</td>\n",
       "      <td>bnsf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Word 0     Word 1      Word 2    Word 3     Word 4   Word 5  Word 6  \\\n",
       "Topics                                                                          \n",
       "0           fuel    gallons    released      cars     diesel  spilled     gal   \n",
       "1           cars      track        rail  derailed        car   switch    crew   \n",
       "2            car   derailed       track      cars    causing      end  switch   \n",
       "3             ns   derailed       loads      tons    empties  pulling   units   \n",
       "4            car       cars       track       cut       bowl  humping    hump   \n",
       "5          train  emergency    derailed      crew       went       mp    cars   \n",
       "6          train   engineer         car      cars  conductor     stop   brake   \n",
       "7       derailed       cars       track      rail    pulling    train  broken   \n",
       "8          track       cars      switch      crew       yard     lead     end   \n",
       "9         damage      train  pantograph     track  equipment   struck    wire   \n",
       "\n",
       "            Word 7     Word 8     Word 9  \n",
       "Topics                                    \n",
       "0             tank       lead       bnsf  \n",
       "1           derail    causing      train  \n",
       "2             lead    shoving     derail  \n",
       "3           wheels       cars      train  \n",
       "4           rolled     humped     damage  \n",
       "5        conductor  traveling    stopped  \n",
       "6       derailment       went      slack  \n",
       "7        hazardous       main  materials  \n",
       "8         derailed        cut      lined  \n",
       "9          causing  operating       bnsf  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_train = LDA_trains(reports = train_reports, N_topics = 10, N_words = 10)\n",
    "lda_train.Topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part c\n",
    "\n",
    "Use the class you developed for Problem 1b to obtain the probabilities\n",
    "for each of the topics in the first 10 narratives in the `TrainNarratives.txt`\n",
    "data set. What is the notation in Figure1 that represents these prob-\n",
    "abilities?\n",
    "\n",
    "**Answer**\n",
    "\n",
    "The notation in Figure 1 that represents these probabilities is $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reports</th>\n",
       "      <th>Topic 0</th>\n",
       "      <th>Topic 1</th>\n",
       "      <th>Topic 2</th>\n",
       "      <th>Topic 3</th>\n",
       "      <th>Topic 4</th>\n",
       "      <th>Topic 5</th>\n",
       "      <th>Topic 6</th>\n",
       "      <th>Topic 7</th>\n",
       "      <th>Topic 8</th>\n",
       "      <th>Topic 9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UNITS 231-281(BACK TO BACK)  WERE COMING INTO ...</td>\n",
       "      <td>0.004546</td>\n",
       "      <td>0.004547</td>\n",
       "      <td>0.959082</td>\n",
       "      <td>0.004546</td>\n",
       "      <td>0.004546</td>\n",
       "      <td>0.004546</td>\n",
       "      <td>0.004548</td>\n",
       "      <td>0.004547</td>\n",
       "      <td>0.004546</td>\n",
       "      <td>0.004547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENGINE 286 CAUGHT FIRE AT THE SPRINGFIELD, MA ...</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.009092</td>\n",
       "      <td>0.009093</td>\n",
       "      <td>0.009093</td>\n",
       "      <td>0.208054</td>\n",
       "      <td>0.009093</td>\n",
       "      <td>0.009092</td>\n",
       "      <td>0.009094</td>\n",
       "      <td>0.009092</td>\n",
       "      <td>0.719205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN NO.#4 WITH ENGS 83/11/90/44 AND 11 CARS ...</td>\n",
       "      <td>0.002326</td>\n",
       "      <td>0.098332</td>\n",
       "      <td>0.588089</td>\n",
       "      <td>0.002326</td>\n",
       "      <td>0.002326</td>\n",
       "      <td>0.002326</td>\n",
       "      <td>0.297296</td>\n",
       "      <td>0.002326</td>\n",
       "      <td>0.002326</td>\n",
       "      <td>0.002326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WHILE SHOVING TRAIN 624 SOUTH ON #30 TRACK AT ...</td>\n",
       "      <td>0.006250</td>\n",
       "      <td>0.006251</td>\n",
       "      <td>0.006251</td>\n",
       "      <td>0.006251</td>\n",
       "      <td>0.006251</td>\n",
       "      <td>0.006252</td>\n",
       "      <td>0.006252</td>\n",
       "      <td>0.006252</td>\n",
       "      <td>0.784815</td>\n",
       "      <td>0.165175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN 786 WAS STRUCK BY A FALLING TREE SOUTH O...</td>\n",
       "      <td>0.010001</td>\n",
       "      <td>0.010002</td>\n",
       "      <td>0.010002</td>\n",
       "      <td>0.010001</td>\n",
       "      <td>0.010001</td>\n",
       "      <td>0.909986</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010003</td>\n",
       "      <td>0.010001</td>\n",
       "      <td>0.010003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ENGINE 4403 OF NJT TRAIN 3204 HAD 90% OLD BREA...</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.954998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AGR CREW DELIVERED CARS TO BNSF AT BNSF YARD A...</td>\n",
       "      <td>0.002273</td>\n",
       "      <td>0.002273</td>\n",
       "      <td>0.247247</td>\n",
       "      <td>0.002273</td>\n",
       "      <td>0.262689</td>\n",
       "      <td>0.002273</td>\n",
       "      <td>0.002273</td>\n",
       "      <td>0.002273</td>\n",
       "      <td>0.347539</td>\n",
       "      <td>0.128886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TRAIN #263 CAME INTO RUTHLAND YARD AND THEY WE...</td>\n",
       "      <td>0.002083</td>\n",
       "      <td>0.821133</td>\n",
       "      <td>0.002084</td>\n",
       "      <td>0.002084</td>\n",
       "      <td>0.002084</td>\n",
       "      <td>0.114560</td>\n",
       "      <td>0.049722</td>\n",
       "      <td>0.002084</td>\n",
       "      <td>0.002084</td>\n",
       "      <td>0.002084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CREW WAS SHOVING A CUT OF CARS EASTWARD TOWARD...</td>\n",
       "      <td>0.003847</td>\n",
       "      <td>0.273505</td>\n",
       "      <td>0.003847</td>\n",
       "      <td>0.003847</td>\n",
       "      <td>0.003847</td>\n",
       "      <td>0.003847</td>\n",
       "      <td>0.003847</td>\n",
       "      <td>0.003847</td>\n",
       "      <td>0.695719</td>\n",
       "      <td>0.003847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>WHILE BUILDING TAIN, 1130 TRIMMER DERAILED FOU...</td>\n",
       "      <td>0.251954</td>\n",
       "      <td>0.007694</td>\n",
       "      <td>0.551449</td>\n",
       "      <td>0.007694</td>\n",
       "      <td>0.007695</td>\n",
       "      <td>0.007693</td>\n",
       "      <td>0.007693</td>\n",
       "      <td>0.142741</td>\n",
       "      <td>0.007694</td>\n",
       "      <td>0.007693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Reports   Topic 0   Topic 1  \\\n",
       "0  UNITS 231-281(BACK TO BACK)  WERE COMING INTO ...  0.004546  0.004547   \n",
       "1  ENGINE 286 CAUGHT FIRE AT THE SPRINGFIELD, MA ...  0.009091  0.009092   \n",
       "2  TRAIN NO.#4 WITH ENGS 83/11/90/44 AND 11 CARS ...  0.002326  0.098332   \n",
       "3  WHILE SHOVING TRAIN 624 SOUTH ON #30 TRACK AT ...  0.006250  0.006251   \n",
       "4  TRAIN 786 WAS STRUCK BY A FALLING TREE SOUTH O...  0.010001  0.010002   \n",
       "5  ENGINE 4403 OF NJT TRAIN 3204 HAD 90% OLD BREA...  0.005000  0.005000   \n",
       "6  AGR CREW DELIVERED CARS TO BNSF AT BNSF YARD A...  0.002273  0.002273   \n",
       "7  TRAIN #263 CAME INTO RUTHLAND YARD AND THEY WE...  0.002083  0.821133   \n",
       "8  CREW WAS SHOVING A CUT OF CARS EASTWARD TOWARD...  0.003847  0.273505   \n",
       "9  WHILE BUILDING TAIN, 1130 TRIMMER DERAILED FOU...  0.251954  0.007694   \n",
       "\n",
       "    Topic 2   Topic 3   Topic 4   Topic 5   Topic 6   Topic 7   Topic 8  \\\n",
       "0  0.959082  0.004546  0.004546  0.004546  0.004548  0.004547  0.004546   \n",
       "1  0.009093  0.009093  0.208054  0.009093  0.009092  0.009094  0.009092   \n",
       "2  0.588089  0.002326  0.002326  0.002326  0.297296  0.002326  0.002326   \n",
       "3  0.006251  0.006251  0.006251  0.006252  0.006252  0.006252  0.784815   \n",
       "4  0.010002  0.010001  0.010001  0.909986  0.010000  0.010003  0.010001   \n",
       "5  0.005000  0.005000  0.005000  0.005000  0.005000  0.005000  0.005000   \n",
       "6  0.247247  0.002273  0.262689  0.002273  0.002273  0.002273  0.347539   \n",
       "7  0.002084  0.002084  0.002084  0.114560  0.049722  0.002084  0.002084   \n",
       "8  0.003847  0.003847  0.003847  0.003847  0.003847  0.003847  0.695719   \n",
       "9  0.551449  0.007694  0.007695  0.007693  0.007693  0.142741  0.007694   \n",
       "\n",
       "    Topic 9  \n",
       "0  0.004547  \n",
       "1  0.719205  \n",
       "2  0.002326  \n",
       "3  0.165175  \n",
       "4  0.010003  \n",
       "5  0.954998  \n",
       "6  0.128886  \n",
       "7  0.002084  \n",
       "8  0.003847  \n",
       "9  0.007693  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_train.Predict_Topics(train_reports[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part d\n",
    "\n",
    "Briefly explain how a safety engineer at Federal Railroad Administration could use the results you obtain in Problem 1c to improve safety for trains.\n",
    "\n",
    "**Answer**\n",
    "\n",
    "The safety engineer at Federal Rialroad Administration could look at the train report and the topic that report belongs to. Then, the safety engineer will be able to review the words under that topic and pay special attention to those areas when he/she conduct the safety checks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "(10) Neural network are a graphical model, Markov Networks, that can\n",
    "be analyzed with Bayesian methods and Boltzmann machines are examples. Provide an explanation for the intractability of the calculation of the\n",
    "partition function, Z, in a Boltzmann machine. Explain how restricted Boltzmann machines help with this problem. Also explain how MCMC\n",
    "and variational methods can provide approximations to Z in restricted\n",
    "Boltzman machines\n",
    "\n",
    "**Answer**\n",
    "\n",
    "In an unrestricted Boltzmann machines, the hidden units are connected and the observed units are also connected. We defined the partition function, $Z$, as the sum of products of the maximal cliques. The computation of the potential function at each maximal cliques will grow exponentially because the hidden units and the observed units are connected with each other.  \n",
    "\n",
    "By removing the connections among the hidden units and the connections among observed units, the number of maximal cliques will decrease dramatically. Now, the fully connected units will only be pairs, one hidden node to one observed node. In the restricted Bltzmann machine, the number of maximal cliques will be the number of hidden units times the number of observed units. \n",
    "\n",
    "We can use MCMC to do a samping on the cliques and use that to estimate the sample probabilities. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3\n",
    "\n",
    "(30) The following questions are based on reading and running the jupyter\n",
    "notebook, `pymc3-variation-inference-neural-network.ipynb`,\n",
    "by Thomas Wiecki, updated by Maxim Kochurov as provided in their blog\n",
    "post. Run the notebook and then answer these questions.\n",
    "\n",
    "####  Part a\n",
    "\n",
    "Wieki says that an advantage to using Bayesian modeling with neural\n",
    "network and deep learning is that \"we could train the model specifically on samples it is most uncertain about.\" Explain how he finds\n",
    "these samples in this example. Explain how you would implement\n",
    "his suggestion (you do not have to actually implement this).\n",
    "\n",
    "**Answer**\n",
    "\n",
    "Thomas Wiecki calculates the standard deviation of the posterior predictive to get a sense for the uncertainty in his predictions. He also created a heapmap based on the standard deviation of each posterior prediction to get a sense of where the predictions surface is with high standard deviations.  \n",
    "\n",
    "With the standard deviation calculated, I would separated the data with highest uncertainty out, for example top 25% of the data with highest standard deviation on posterior prediction. Then, I would build a new model on those 25% of the data. Once I have the new model, I will refit the entire dataset (100%) using the new model to get a new posterior prediction. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part b\n",
    "\n",
    "Wieki also says that another advantage to Bayesian modeling with\n",
    "neural network and deep learning is that \\We also get uncertainty\n",
    "estimates of our weights which could inform us about the stability\n",
    "of the learned representations of the network.\" Discuss what the\n",
    "uncertainty estimates for the weights found for the example in this\n",
    "notebook imply.\n",
    "\n",
    "**Answer**\n",
    "\n",
    "Different layers of the neural network has different weights for the same predictor. We can view the uncertainty estimates for the weights same as the way we viewed the confidence interval for regression coefficients. If the uncertainty estimates for the weights covers the value of 0, that implies that such weights may not be useful in our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part c\n",
    "\n",
    "Explain how the Gaussian priors help to regularize the weights in the\n",
    "neural network.\n",
    "\n",
    "**Answer**\n",
    "\n",
    "Since we are using MAP estimation for the posterior distribution, we are trying to maximize the probability of the parameter given the data: \n",
    "\n",
    "$$P(W|x, y) = \\frac{P(x, y | W) P(W)}{P(x ,y)} $$\n",
    "\n",
    "where $W$ is the weights of predictor $x$, and $y$ is the response variable.\n",
    "\n",
    "Since $P(x, y)$ is the normalizing constant, we can ignore that part. The maximum log likelihood then become \n",
    "\n",
    "$$log(P(W|x, y)) = log(P(x, y | W)) +  log(P(W))$$\n",
    "\n",
    "If we assume the Gaussian priors, each element of the weights are drawn independently from a unit Gaussian, then \n",
    "\n",
    "$$log(P(W)) = log( \\frac{1}{\\sqrt{2\\pi}} \\prod_{ij} exp(- \\frac{w_{ij}^2}{2} )) = -\\frac{1}{2} \\sum_{ij} w_{ij}^2 +  log( \\frac{1}{\\sqrt{2\\pi}}) $$\n",
    "\n",
    "Now the log likelihood becomes \n",
    "\n",
    "$$log(P(W|x, y)) = log(P(x, y | W)) - \\frac{1}{2} \\sum_{ij} w_{ij}^2 + c$$\n",
    "where $c = log( \\frac{1}{\\sqrt{2\\pi}})$, which is a constant.\n",
    "\n",
    "Therefore, the regularitzation penalty $R(W) = \\frac{1}{2} \\sum_{ij} w_{ij}^2$.\n",
    "\n",
    "\n",
    "Reference: https://stats.stackexchange.com/questions/229415/why-is-regularization-interpreted-as-a-gaussian-prior-on-my-weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part d\n",
    "\n",
    "Why do we use a variational apprxomiation instead of sampling for estimating the posterior of the weights? \n",
    "\n",
    "**Answer**\n",
    "\n",
    "Because the sampling method cannot scale to very large data sets and it can be very slow when working with high dimensional parameters or latent space. The posterior distribution may also be very complex to draw samples from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part e\n",
    "\n",
    "Change the prior distributinos for all three sets of the neural net weights to Cauchy with location (alpha) = 0 and scale (beta) = 2. Rerun the remaining cells in the notebook and comment on any changes you see from this. \n",
    "\n",
    "**Answer**\n",
    "- The Average Loss coming out of the ADVI model is higher.\n",
    "- The toal prediction accuracy becomes better, changing from 95% to 96.4%\n",
    "- The final Minibatch-ADVI model now have 3 weights that does not contain 0 in its corresponding distribution. Previously, there were only 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4\n",
    "\n",
    "(30) You are tracking the performance of a set of companies with the idea\n",
    "that you might possibly buy stock in them. You decide to automate this process using HMM and you implement your first version for one company.\n",
    "This company has three states that are hidden from investors: (1) in-trouble; (2) static; and (3) major growth potential. You have estimated\n",
    "the transition probabilities between states as follows:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    ".6 & .3 & .1\\\\ \n",
    ".4 & .4 & .2\\\\ \n",
    ".1 & .4 & .5\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "You have a text analysis system using Naive Bayes to process the quarterly\n",
    "reports and assess their sentiment into one of three categories: (1) Fine;\n",
    "(2) Good; and (3) Very good. Your estimates for the probabilities of these\n",
    "sentiments given the state of the company are shown in the following\n",
    "matrix (the sentiments are in the rows and the states of the company are\n",
    "in the columns).\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    ".45 & .4 & .15\\\\ \n",
    ".3 & .4 & .3\\\\ \n",
    ".2 & .5 & .3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "You have 3 quarterly reports with the assessments: Fine, Fine, Very Good\n",
    "and your prior for the initial state is equally likely for each value. The\n",
    "following questions use the HMM class in the jupyter notebook, HMM\n",
    "Examples HW5 - Burglary and Investment with your additions\n",
    "to it as indicated in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Nov 12 20:35:27 2020\n",
    "\n",
    "@author: donaldbrown\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class HMM:\n",
    "    \"\"\"Creates a class for Hidden Markov Models\n",
    "    Input:\n",
    "        Viz:      List of observed or visible states over time\n",
    "        Trans_M:  Transition matrix for hidden states, H X H, H=len(Trans_M), no. of hidden variables\n",
    "        Obs_M:    Observation matrix, H X V, V = no. of visible variables\n",
    "        Pi:       List of initial state probabilities\n",
    "    Methods:\n",
    "        filter = The posterior probabilities for hidden states for each time period, T X H array\n",
    "        smoother = The probabiliteis for the hidden states at each prevoius time period, T X H array\n",
    "        viturbi = The most likely path of hidden states given the observed state, data frame, 1 X T\n",
    "        predictor = The probabilities for next hidden state and the next observed state, 1 X H array \"\"\"\n",
    "    \n",
    "    def __init__(self,Viz, Trans_M, Obs_M, Pi):\n",
    "        # initialize variables\n",
    "        # Hidden state transition matrix\n",
    "        self.Trans_M = Trans_M\n",
    "        # Visible or observates state probabilities given the hidden states\n",
    "        self.Obs_M = Obs_M\n",
    "        # No. of hidden states\n",
    "        self.H = Trans_M.shape[0]\n",
    "        # No. of observed states\n",
    "        self.V = Obs_M.shape[0]\n",
    "        # prior probabaiities for the hidden states\n",
    "        self.Pi = Pi\n",
    "        # List of observed states over time\n",
    "        self.Viz = Viz\n",
    "\n",
    "        \n",
    "    def filter(self):\n",
    "        \n",
    "        T = len(self.Viz)\n",
    "        \n",
    "        # Obtain the joint probabilities of the hidden and observed states at time t\n",
    "        self.alpha = np.zeros((T, self.H))\n",
    "        self.alpha[0, :] = self.Pi * self.Obs_M[:,self.Viz[0]]\n",
    " \n",
    "        for t in range(1, T):\n",
    "            for j in range(self.H):\n",
    "                self.alpha[t, j] = self.alpha[t - 1].dot(self.Trans_M[:, j]) * self.Obs_M[j, self.Viz[t]]\n",
    "        \n",
    "        ### Insert your code here to computer the posterior probabilities ###\n",
    "        self.Post = np.zeros((T, self.H))\n",
    "        \n",
    "        for t in range(0, T):\n",
    "            sum_of_row = sum(self.alpha[t,:])\n",
    "            for j in range(self.H):\n",
    "                self.Post[t, j] = self.alpha[t, j] / sum_of_row\n",
    "\n",
    "        print(\"self.alpha\")\n",
    "        print(self.alpha)\n",
    "        print(\"Posterior\")\n",
    "        print(self.Post)   \n",
    "        return self.Post\n",
    "      \n",
    "    def smoother(self):\n",
    "\n",
    "        T = len(self.Viz)\n",
    "        self.beta = np.zeros((T, self.H))\n",
    " \n",
    "        # setting beta(T) = 1\n",
    "        self.beta[T - 1] = np.ones((self.H))\n",
    " \n",
    "        # Loop backwards way from T-1 to 1\n",
    "        # Due to python indexing the actual loop will be T-2 to 0\n",
    "        for t in range(T - 2, -1, -1):\n",
    "            for j in range(self.H):\n",
    "                self.beta[t, j] = (self.beta[t + 1] * self.Obs_M[:, self.Viz[t + 1]]).dot(self.Trans_M[j, :])\n",
    "                \n",
    "        # Obtain the posterior probabilities of the hidden states given the observed states       \n",
    "        \n",
    "        ### Insert your code here to compute the posterior probabilities ###    \n",
    "        self.Post_smoother = np.zeros((T, self.H))\n",
    "        \n",
    "        for t in range(0, T):\n",
    "            sum_of_row = sum(self.alpha[t] * self.beta[t])\n",
    "            \n",
    "            for j in range(self.H):\n",
    "                self.Post_smoother[t, j] = (self.alpha[t, j] * self.beta[t, j])  / sum_of_row      \n",
    "        \n",
    "        print(\"beta\")\n",
    "        print(self.beta)\n",
    "        print(\"Posterior\")\n",
    "        print(self.Post_smoother)\n",
    " \n",
    "        return self.Post_smoother\n",
    "    \n",
    "    \n",
    "    def viturbi(self):\n",
    "        T = len(self.Viz)\n",
    "        \n",
    "        # Obtain the joint probabity of the most likely path that ends in state j at time t\n",
    "        delta = np.zeros((T, self.H))\n",
    "        delta[0, :] = (self.Pi * self.Obs_M[:, Viz[0]])\n",
    " \n",
    "\n",
    "        prev = np.zeros((T, self.H))\n",
    "        prev[0,:] = np.repeat(None, 3)\n",
    " \n",
    "        for t in range(1, T):\n",
    "            for j in range(self.H):\n",
    "                # The most likely state given our previoius state at t-1\n",
    "                \n",
    "                prob = delta[t - 1] * (self.Trans_M[:, j])\n",
    " \n",
    "                #  The probability of the most probable state given the previous state and the observation at time t\n",
    "                \n",
    "                delta[t, j] = np.max(prob) * (self.Obs_M[j, Viz[t]])                \n",
    "                \n",
    "                # The most probable state given previous state \n",
    "\n",
    "                prev[t, j] = np.argmax(prob)\n",
    " \n",
    "                \n",
    "        # Path Array\n",
    "        S = np.zeros(T)\n",
    " \n",
    "        # Find the most probable last hidden state\n",
    "        last_state = np.argmax(delta[T-1, :])\n",
    " \n",
    "        S[T-1] = last_state\n",
    "        \n",
    "        # Find the most probable hidden states at the previous times\n",
    "        # Walk backwords\n",
    "        ### Insert your code here ###\n",
    "        for t in range(T-1, 0, -1):\n",
    "            S[t] = np.argmax(delta[t, :])\n",
    "            \n",
    "        # Change to states numbers in problem (i.e., +1)\n",
    "        S = S+1\n",
    "            \n",
    "        S = S.reshape([1,3])\n",
    " \n",
    "        # Path, S, as a dataframe \n",
    "        # Create a list of column names, Time  \n",
    "        cols = list()\n",
    "        for i in range(1,T+1):\n",
    "            cols.append(\"Time \"+(str(i)))\n",
    "        Path = pd.DataFrame(S, columns = cols)\n",
    "        print('delta')\n",
    "        print(delta)\n",
    "        print('Previous')\n",
    "        print(prev)        \n",
    "        print(\"Path\")\n",
    "        print(Path)\n",
    "        return Path\n",
    " \n",
    "\n",
    "    def predictor(self, steps = 1):\n",
    "        T = len(self.Viz)\n",
    "        # Hidden state prediction probabilities using filtering results (Post)\n",
    "        ### Insert your code here ### dot product of filter post and transition matrix\n",
    "        Pred_Hidden = Pred_Hidden = self.Post[T-1, :] @ self.Trans_M \n",
    "        print(\"Predicted Hidden State\")\n",
    "        print(Pred_Hidden)\n",
    "        # Visible state prediction using the predicted hidden state probabilities\n",
    "        ### Insert your code here ### dot product pred_hidden with observed matrix\n",
    "        Pred_Visible = Pred_Visible = Pred_Hidden @ self.Obs_M\n",
    "        print(\"Predicted Visible State\")\n",
    "        print(Pred_Visible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transition matrix\n",
    "TM = np.array([[.6,.3,.1],[.4,.4,.2],[.1,.4,.5]])  \n",
    "# Observation matrix\n",
    "OM = np.array([[.45,.4,.15],[.3,.4,.3],[.2,.5,.3]])\n",
    "OM = OM.T\n",
    "# # Prior probabilities of hidden states\n",
    "p = [1/3, 1/3, 1/3]\n",
    "# # Observed visible states\n",
    "Viz = [0, 0, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part a \n",
    "In order to decide whether to invest, find the most likely current state\n",
    "given the observed states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.alpha\n",
      "[[0.15       0.13333333 0.05      ]\n",
      " [0.06675    0.04733333 0.01      ]\n",
      " [0.01199667 0.02147917 0.0063425 ]]\n",
      "Posterior\n",
      "[[0.45       0.4        0.15      ]\n",
      " [0.53794493 0.38146407 0.080591  ]\n",
      " [0.301285   0.53942907 0.15928592]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.45      , 0.4       , 0.15      ],\n",
       "       [0.53794493, 0.38146407, 0.080591  ],\n",
       "       [0.301285  , 0.53942907, 0.15928592]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use Filter to infer the present: get the current state\n",
    "hmm1 = HMM(Viz, TM, OM, p)\n",
    "hmm1.filter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most likely current state given the observed states is **Static**.\n",
    "\n",
    "#### Part b\n",
    "\n",
    "(b) Using smoothing to find the most likely state at each previous time\n",
    "period (i.e., periods 1 and 2).."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta\n",
      "[[0.12735 0.1195  0.09565]\n",
      " [0.3     0.34    0.37   ]\n",
      " [1.      1.      1.     ]]\n",
      "Posterior\n",
      "[[0.47974133 0.40015068 0.12010799]\n",
      " [0.50290905 0.40416893 0.09292202]\n",
      " [0.301285   0.53942907 0.15928592]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.47974133, 0.40015068, 0.12010799],\n",
       "       [0.50290905, 0.40416893, 0.09292202],\n",
       "       [0.301285  , 0.53942907, 0.15928592]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm1.smoother()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most likely state at time period 1 and 2 is **In Trouble, In Trouble**\n",
    "\n",
    "#### Part c\n",
    "\n",
    "Show the most likely path of performance through the hidden states\n",
    "up to the current time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta\n",
      "[[0.15       0.13333333 0.05      ]\n",
      " [0.0405     0.02133333 0.004     ]\n",
      " [0.00486    0.006075   0.00128   ]]\n",
      "Previous\n",
      "[[nan nan nan]\n",
      " [ 0.  1.  1.]\n",
      " [ 0.  0.  1.]]\n",
      "Path\n",
      "   Time 1  Time 2  Time 3\n",
      "0     1.0     1.0     2.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time 1</th>\n",
       "      <th>Time 2</th>\n",
       "      <th>Time 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time 1  Time 2  Time 3\n",
       "0     1.0     1.0     2.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm1.viturbi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most likely path of perofrmance through the hidden states up to the current time is **In Trouble, In Trouble, Static**.\n",
    "\n",
    "#### Part d\n",
    "\n",
    "Find the most ikely hidden state and visible state for this company\n",
    "in the next time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Hidden State\n",
      "[0.41247122 0.3698715  0.21765728]\n",
      "Predicted Visible State\n",
      "[0.36620924 0.33698715 0.33272718]\n"
     ]
    }
   ],
   "source": [
    "hmm1.predictor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The most ikely hidden state for this company in the next time period is **In Trouble**.\n",
    "- The most ikely visible state for this company in the next time period is **Fine**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
